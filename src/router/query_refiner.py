"""
Pipeline de refinamento de queries para busca vetorial.

Funcionalidades:
- Recebe uma query, gera embedding
- Compara com embeddings de queries anteriores que tiveram sucesso (mem√≥ria)
- **NOVO:** Gera paraphrases via LLM (LangChain) para aumentar recall
- Se similaridade abaixo do limiar, gera uma nova query refinada (simplesmente por heur√≠stica + ontologia)
- Repete at√© atingir qualidade m√≠nima ou n√∫mero m√°ximo de itera√ß√µes

Integra com: EmbeddingGenerator, Memory (Supabase), LLM Manager (LangChain) e Logging
"""
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field
import time
import re

from src.embeddings.generator import EmbeddingGenerator, EmbeddingProvider
from src.embeddings.vector_store import VectorStore
from src.utils.logging_config import get_logger
from src.router.semantic_ontology import StatisticalOntology

# Import LLM Manager para paraphrases
try:
    from src.llm.manager import LLMManager, LLMConfig
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    LLMManager = None
    LLMConfig = None

logger = get_logger(__name__)


@dataclass
class RefinementResult:
    query: str
    embedding: List[float]
    similarity_to_best: float
    iterations: int
    success: bool
    # M√©tricas de paraphrase (novo)
    paraphrase_used: bool = False
    num_paraphrases_tested: int = 0
    best_paraphrase_similarity: float = 0.0
    llm_model_id: Optional[str] = None
    paraphrase_elapsed_time: float = 0.0
    paraphrases_generated: List[str] = field(default_factory=list)


class QueryRefiner:
    """Implementa loop de refinamento de queries baseado em embeddings hist√≥ricos.

    Nota: essa implementa√ß√£o usa heur√≠sticas simples (ontologia + pequenas substitui√ß√µes)
    e busca por queries hist√≥ricas salvas na mem√≥ria (SupabaseMemory) que tenham sido
    marcadas como bem-sucedidas.
    """

    def __init__(self,
                 embedding_provider: EmbeddingProvider = EmbeddingProvider.SENTENCE_TRANSFORMER,
                 memory: Optional[VectorStore] = None,
                 similarity_threshold: float = 0.72,
                 max_iterations: int = 3,
                 enable_paraphrase: bool = True,
                 num_paraphrases: int = 3):
        self.embedding_gen = EmbeddingGenerator(provider=embedding_provider)
        # usar VectorStore (s√≠ncrono) para hist√≥rico de queries/embeddings
        self.memory = memory or VectorStore()
        self.similarity_threshold = similarity_threshold
        self.max_iterations = max_iterations
        
        # Paraphrase via LLM (novo)
        self.enable_paraphrase = enable_paraphrase and LLM_AVAILABLE
        self.num_paraphrases = num_paraphrases
        self.llm_manager = None
        
        if self.enable_paraphrase:
            try:
                self.llm_manager = LLMManager()
                logger.info("üî§ QueryRefiner: Paraphrase via LLM habilitado (N=%d)", self.num_paraphrases)
            except Exception as e:
                logger.warning("‚ö†Ô∏è Falha ao inicializar LLM Manager para paraphrases: %s", str(e))
                self.enable_paraphrase = False
        else:
            if not LLM_AVAILABLE:
                logger.warning("‚ö†Ô∏è LLM Manager indispon√≠vel - paraphrases desabilitados")

    def _get_best_historical_similarity(self, query_emb: List[float]) -> float:
        """Consulta a mem√≥ria por queries hist√≥ricas e retorna a maior similaridade encontrada."""
        try:
            # VectorStore.search_similar retorna VectorSearchResult com similarity_score
            results = self.memory.search_similar(query_embedding=query_emb, similarity_threshold=0.0, limit=10)
            if not results:
                return 0.0
            best = max(r.similarity_score for r in results)
            return best
        except Exception as e:
            logger.warning("Falha ao buscar hist√≥rico de queries: %s", str(e))
            return 0.0

    def _heuristic_refine(self, query: str, iteration: int) -> str:
        """Gera uma refinamento simples da query usando a ontologia e regras b√°sicas."""
        # usar ontologia para detectar inten√ß√£o e acrescentar termos relevantes
        parts = StatisticalOntology.expand_query(query)
        refined = query
        # Em itera√ß√µes posteriores, acrescentar termos expl√≠citos
        if parts.get('variability'):
            refined = refined + ' desvio padr√£o vari√¢ncia'
        if parts.get('central_tendency'):
            refined = refined + ' m√©dia mediana'
        if parts.get('interval'):
            refined = refined + ' m√≠nimo m√°ximo range'

        # small change per iteration to diversify
        if iteration == 1:
            refined = refined + ' detalhes por vari√°vel'
        elif iteration >= 2:
            refined = refined + ' incluir exemplos e colunas espec√≠ficas'

        return refined
    
    def _generate_paraphrases_via_llm(self, query: str) -> List[str]:
        """Gera N paraphrases da query usando LLM via LangChain.
        
        Args:
            query: Query original
            
        Returns:
            Lista de paraphrases (pode estar vazia se falhar)
        """
        if not self.enable_paraphrase or not self.llm_manager:
            return []
        
        try:
            # Prompt seguro para paraphrase
            system_prompt = (
                "Voc√™ √© um especialista em reformula√ß√£o de perguntas t√©cnicas sobre an√°lise de dados estat√≠sticos. "
                "Sua tarefa √© gerar varia√ß√µes lingu√≠sticas que mantenham fielmente a inten√ß√£o original."
            )
            
            user_prompt = f"""Reformule a seguinte pergunta de an√°lise de dados, mantendo fielmente a inten√ß√£o original, mas usando palavras diferentes, sin√¥nimos ou estruturas alternativas do dom√≠nio estat√≠stico:

"{query}"

Produza exatamente {self.num_paraphrases} varia√ß√µes distintas, uma por linha, sem numera√ß√£o ou marcadores."""
            
            # Configura√ß√£o LLM: temperatura baixa para manter coer√™ncia
            config = LLMConfig(
                temperature=0.3,  # baixa para evitar criatividade excessiva
                max_tokens=512,
                top_p=0.9
            )
            
            logger.info("üî§ Gerando %d paraphrases via LLM...", self.num_paraphrases)
            start = time.time()
            
            response = self.llm_manager.chat(
                prompt=user_prompt,
                config=config,
                system_prompt=system_prompt
            )
            
            elapsed = time.time() - start
            
            if not response.success:
                logger.warning("‚ö†Ô∏è Falha ao gerar paraphrases: %s", response.error)
                return []
            
            # Parsear resposta: espera-se N linhas n√£o-vazias
            paraphrases = []
            lines = response.content.strip().split('\n')
            for line in lines:
                # Remover numera√ß√£o/marcadores se presentes (ex: "1.", "‚Ä¢", etc)
                cleaned = re.sub(r'^[\d\.\-\*‚Ä¢\s]+', '', line).strip()
                if cleaned and len(cleaned) > 10:  # filtrar linhas muito curtas
                    paraphrases.append(cleaned)
            
            # Limitar ao n√∫mero solicitado
            paraphrases = paraphrases[:self.num_paraphrases]
            
            logger.info(
                "‚úÖ Paraphrases geradas: %d varia√ß√µes em %.2fs usando modelo %s",
                len(paraphrases), elapsed, response.model
            )
            
            for i, p in enumerate(paraphrases, 1):
                logger.debug("  Paraphrase %d: %s", i, p[:80] + ('...' if len(p) > 80 else ''))
            
            return paraphrases
            
        except Exception as e:
            logger.error("‚ùå Erro ao gerar paraphrases via LLM: %s", str(e))
            return []
    
    def _test_paraphrases(self, paraphrases: List[str]) -> Dict[str, Any]:
        """Testa cada paraphrase na busca vetorial e retorna a melhor.
        
        Args:
            paraphrases: Lista de queries parafraseadas
            
        Returns:
            Dict com best_query, best_embedding, best_similarity, tested_count
        """
        if not paraphrases:
            return {
                'best_query': None,
                'best_embedding': None,
                'best_similarity': 0.0,
                'tested_count': 0
            }
        
        best_similarity = 0.0
        best_query = None
        best_embedding = None
        
        logger.info("üîç Testando %d paraphrases na busca vetorial...", len(paraphrases))
        
        for i, paraphrase in enumerate(paraphrases, 1):
            try:
                # Gerar embedding para paraphrase
                emb_result = self.embedding_gen.generate_embedding(paraphrase)
                query_emb = emb_result.embedding
                
                # Buscar similaridade com hist√≥rico
                similarity = self._get_best_historical_similarity(query_emb)
                
                logger.debug(
                    "  Paraphrase %d: similarity=%.3f query='%s'",
                    i, similarity, paraphrase[:60] + ('...' if len(paraphrase) > 60 else '')
                )
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_query = paraphrase
                    best_embedding = query_emb
                    
            except Exception as e:
                logger.warning("‚ö†Ô∏è Erro ao testar paraphrase %d: %s", i, str(e))
                continue
        
        if best_query:
            logger.info(
                "‚úÖ Melhor paraphrase: similarity=%.3f query='%s'",
                best_similarity, best_query[:80] + ('...' if len(best_query) > 80 else '')
            )
        else:
            logger.warning("‚ö†Ô∏è Nenhuma paraphrase produziu resultado v√°lido")
        
        return {
            'best_query': best_query,
            'best_embedding': best_embedding,
            'best_similarity': best_similarity,
            'tested_count': len(paraphrases)
        }

    def refine_query(self, original_query: str) -> RefinementResult:
        """Executa o loop de refinamento com paraphrases via LLM.
        
        Fluxo:
        1. Testa query original
        2. Se falhar E paraphrase habilitado: gera paraphrases via LLM e testa cada uma
        3. Se melhor paraphrase atingir limiar: retorna sucesso
        4. Se paraphrases falharem: continua com heur√≠sticas (ontologia)
        5. Se tudo falhar: retorna √∫ltima tentativa com success=False

        Retorna RefinementResult com dados da √∫ltima itera√ß√£o e m√©tricas de paraphrase.
        """
        start = time.time()
        query = original_query
        iterations = 0
        success = False
        best_similarity = 0.0
        
        # M√©tricas de paraphrase
        paraphrase_used = False
        num_paraphrases_tested = 0
        best_paraphrase_similarity = 0.0
        llm_model_id = None
        paraphrase_elapsed_time = 0.0
        paraphrases_generated = []
        query_emb = []

        try:
            # === ETAPA 1: Testar query original ===
            iterations += 1
            emb_result = self.embedding_gen.generate_embedding(original_query)
            query_emb = emb_result.embedding
            best_similarity = self._get_best_historical_similarity(query_emb)
            
            logger.info(
                "[QueryRefiner] Iter %d (original): best_historical_similarity=%.3f for query='%s'",
                iterations, best_similarity, original_query
            )
            
            if best_similarity >= self.similarity_threshold:
                success = True
                elapsed = time.time() - start
                logger.info(
                    "[QueryRefiner] ‚úÖ Query original atingiu limiar (%.3f >= %.3f) - sem necessidade de refinamento",
                    best_similarity, self.similarity_threshold
                )
                return RefinementResult(
                    query=original_query,
                    embedding=query_emb,
                    similarity_to_best=best_similarity,
                    iterations=iterations,
                    success=success
                )
            
            # === ETAPA 2: Tentar paraphrases via LLM ===
            if self.enable_paraphrase and self.llm_manager:
                logger.info("üî§ Query original n√£o atingiu limiar - tentando paraphrases via LLM...")
                paraphrase_start = time.time()
                
                # Gerar paraphrases
                paraphrases_generated = self._generate_paraphrases_via_llm(original_query)
                
                if paraphrases_generated:
                    # Testar cada paraphrase
                    paraphrase_result = self._test_paraphrases(paraphrases_generated)
                    
                    num_paraphrases_tested = paraphrase_result['tested_count']
                    best_paraphrase_similarity = paraphrase_result['best_similarity']
                    
                    # Capturar modelo LLM usado
                    if self.llm_manager.active_provider:
                        llm_model_id = f"{self.llm_manager.active_provider.value}:{self.llm_manager._get_default_model(self.llm_manager.active_provider)}"
                    
                    paraphrase_elapsed_time = time.time() - paraphrase_start
                    
                    # Se melhor paraphrase atingiu limiar, usar
                    if best_paraphrase_similarity >= self.similarity_threshold:
                        query = paraphrase_result['best_query']
                        query_emb = paraphrase_result['best_embedding']
                        best_similarity = best_paraphrase_similarity
                        paraphrase_used = True
                        success = True
                        
                        logger.info(
                            "‚úÖ Paraphrase bem-sucedida: similarity=%.3f (limiar=%.3f) em %.2fs",
                            best_similarity, self.similarity_threshold, paraphrase_elapsed_time
                        )
                        
                        elapsed = time.time() - start
                        return RefinementResult(
                            query=query,
                            embedding=query_emb,
                            similarity_to_best=best_similarity,
                            iterations=iterations,
                            success=success,
                            paraphrase_used=paraphrase_used,
                            num_paraphrases_tested=num_paraphrases_tested,
                            best_paraphrase_similarity=best_paraphrase_similarity,
                            llm_model_id=llm_model_id,
                            paraphrase_elapsed_time=paraphrase_elapsed_time,
                            paraphrases_generated=paraphrases_generated
                        )
                    else:
                        logger.warning(
                            "‚ö†Ô∏è Paraphrases falharam (melhor=%.3f < limiar=%.3f) - continuando com heur√≠sticas",
                            best_paraphrase_similarity, self.similarity_threshold
                        )
                else:
                    logger.warning("‚ö†Ô∏è Nenhuma paraphrase gerada - continuando com heur√≠sticas")
            
            # === ETAPA 3: Heur√≠sticas (ontologia) ===
            logger.info("üîÑ Tentando refinamento heur√≠stico (ontologia + termos)...")
            
            while iterations < self.max_iterations:
                iterations += 1
                
                # Refinar query usando heur√≠sticas
                query = self._heuristic_refine(original_query, iterations)
                
                # Gerar embedding
                emb_result = self.embedding_gen.generate_embedding(query)
                query_emb = emb_result.embedding

                # Comparar com hist√≥rico
                best_similarity = self._get_best_historical_similarity(query_emb)
                logger.info(
                    "[QueryRefiner] Iter %d (heuristic): best_historical_similarity=%.3f for query='%s'",
                    iterations, best_similarity, query
                )

                if best_similarity >= self.similarity_threshold:
                    success = True
                    break

            elapsed = time.time() - start
            logger.info(
                "[QueryRefiner] refine_query completed: success=%s iterations=%d elapsed=%.2fs paraphrase_used=%s",
                success, iterations, elapsed, paraphrase_used
            )
            
            return RefinementResult(
                query=query,
                embedding=query_emb,
                similarity_to_best=best_similarity,
                iterations=iterations,
                success=success,
                paraphrase_used=paraphrase_used,
                num_paraphrases_tested=num_paraphrases_tested,
                best_paraphrase_similarity=best_paraphrase_similarity,
                llm_model_id=llm_model_id,
                paraphrase_elapsed_time=paraphrase_elapsed_time,
                paraphrases_generated=paraphrases_generated
            )

        except Exception as e:
            logger.error("[QueryRefiner] erro durante refine_query: %s", str(e))
            return RefinementResult(
                query=original_query,
                embedding=[],
                similarity_to_best=0.0,
                iterations=iterations,
                success=False,
                paraphrase_used=paraphrase_used,
                num_paraphrases_tested=num_paraphrases_tested,
                best_paraphrase_similarity=best_paraphrase_similarity,
                llm_model_id=llm_model_id,
                paraphrase_elapsed_time=paraphrase_elapsed_time,
                paraphrases_generated=paraphrases_generated
            )
