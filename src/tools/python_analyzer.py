"""Ferramenta de an√°lise de dados Python para o sistema multiagente.

‚ö†Ô∏è CONFORMIDADE CR√çTICA: Esta ferramenta deve priorizar dados da tabela embeddings
sobre acesso direto a arquivos CSV para agentes de resposta.

Esta ferramenta permite que agentes executem c√≥digo Python real para
calcular estat√≠sticas precisas dos dados armazenados no Supabase.
"""
from __future__ import annotations
import sys
import os
import inspect
from pathlib import Path

# Adiciona o diret√≥rio raiz do projeto ao PYTHONPATH
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

import pandas as pd
import numpy as np
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass
import io
import traceback
import warnings

# Suprimir warnings desnecess√°rios
warnings.filterwarnings('ignore')

from src.utils.logging_config import get_logger

# Import do cliente Supabase para recupera√ß√£o de dados
try:
    from src.vectorstore.supabase_client import supabase
    SUPABASE_CLIENT_AVAILABLE = True
except ImportError as e:
    SUPABASE_CLIENT_AVAILABLE = False
    supabase = None


class UnauthorizedCSVAccessError(Exception):
    """Exce√ß√£o lan√ßada quando acesso n√£o autorizado a CSV √© detectado."""
    pass


@dataclass
class PythonAnalysisResult:
    """Resultado da execu√ß√£o de an√°lise Python"""
    success: bool
    result: Any
    output: str
    error: Optional[str] = None
    execution_time: float = 0.0


class PythonDataAnalyzer:
    """Ferramenta para execu√ß√£o de c√≥digo Python para an√°lise de dados.
    
    ‚ö†Ô∏è CONFORMIDADE: Prioriza dados da tabela embeddings sobre CSV direto.
    
    Esta classe permite que agentes executem c√≥digo Python real para
    calcular estat√≠sticas precisas dos dados, evitando alucina√ß√µes do LLM.
    """
    
    def __init__(self, caller_agent: Optional[str] = None):
        self.logger = get_logger(__name__)
        
        # Detectar e validar caller_agent
        self.caller_agent = caller_agent or self._detect_caller_agent()
        
        self._setup_secure_environment()
        self.logger.info(f"PythonDataAnalyzer inicializado por: {self.caller_agent}")
    
    def _detect_caller_agent(self) -> str:
        """Detecta qual agente est√° chamando este analisador."""
        frame = inspect.currentframe()
        try:
            # Subir na stack para encontrar o caller
            for i in range(15):  # Limite de seguran√ßa
                frame = frame.f_back
                if frame is None:
                    break
                    
                filename = frame.f_code.co_filename
                
                # Verificar se √© um agente conhecido
                if 'ingestion_agent' in filename:
                    return 'ingestion_agent'
                elif 'orchestrator_agent' in filename:
                    return 'orchestrator_agent'
                elif 'csv_analysis_agent' in filename or 'embeddings_analysis_agent' in filename:
                    return 'analysis_agent'
                elif 'rag_agent' in filename:
                    return 'rag_agent'
                elif 'test_' in filename or '_test' in filename:
                    return 'test_system'
                    
            return 'unknown_caller'
            
        finally:
            del frame
    
    def _validate_csv_access_authorization(self) -> None:
        """Valida se o caller tem autoriza√ß√£o para acessar CSV diretamente."""
        authorized_agents = [
            'ingestion_agent',
            'test_system'  # Para testes
        ]
        
        if self.caller_agent not in authorized_agents:
            error_msg = (
                f"‚ö†Ô∏è VIOLA√á√ÉO DE CONFORMIDADE DETECTADA!\n"
                f"Agente '{self.caller_agent}' tentou usar PythonDataAnalyzer com potencial acesso CSV.\n"
                f"Apenas agentes autorizados podem acessar CSV: {authorized_agents}\n"
                f"Agentes de resposta devem usar APENAS a tabela embeddings."
            )
            self.logger.error(error_msg)
            raise UnauthorizedCSVAccessError(error_msg)
        
        self.logger.info(f"‚úÖ PythonDataAnalyzer: Acesso autorizado para agente: {self.caller_agent}")
    
    def _setup_secure_environment(self):
        """Configura ambiente seguro para execu√ß√£o de c√≥digo Python"""
        # Lista de m√≥dulos permitidos (seguran√ßa)
        self.allowed_modules = {
            'pandas', 'numpy', 'math', 'statistics', 'datetime', 'json',
            'collections', 're', 'itertools', 'functools'
        }
        
        # Namespaces seguros para execu√ß√£o
        self.safe_globals = {
            '__builtins__': {
                'len', 'sum', 'min', 'max', 'abs', 'round', 'sorted',
                'enumerate', 'zip', 'range', 'list', 'dict', 'tuple', 'set',
                'str', 'int', 'float', 'bool', 'print'
            },
            'pd': pd,
            'np': np,
        }
    
    def get_data_from_embeddings(self, limit: int = None, metadata_filter: Dict = None, parse_chunk_text: bool = True) -> Optional[pd.DataFrame]:
        """Recupera dados APENAS da tabela embeddings (CONFORMIDADE).
        
        Args:
            limit: Limite de registros (None para todos)
            metadata_filter: Filtros por metadata
            parse_chunk_text: Se True, parseia o conte√∫do CSV do chunk_text para reconstruir colunas originais (PADR√ÉO: True)
            
        Returns:
            DataFrame com os dados PARSEADOS do CSV original ou None se falhar
        """
        if not SUPABASE_CLIENT_AVAILABLE or not supabase:
            self.logger.error("Cliente Supabase n√£o dispon√≠vel")
            return None
        
        try:
            self.logger.info("‚úÖ Recuperando dados da tabela embeddings (CONFORMIDADE)")
            
            query = supabase.table('embeddings').select('*')
            
            # Aplicar filtros se especificados
            if metadata_filter:
                for key, value in metadata_filter.items():
                    query = query.eq(f'metadata->{key}', value)
            
            if limit:
                query = query.limit(limit)
            
            result = query.execute()
            
            if not result.data:
                self.logger.warning("Nenhum dado encontrado na tabela embeddings")
                return None
            
            df = pd.DataFrame(result.data)
            self.logger.info(f"‚úÖ Dados recuperados: {len(df)} registros da tabela embeddings")
            
            # SEMPRE tentar parsear chunk_text para reconstruir dados originais do CSV
            if 'chunk_text' in df.columns:
                self.logger.info("üîÑ Parseando chunk_text para reconstruir colunas originais do CSV...")
                parsed_df = self._parse_chunk_text_to_dataframe(df)
                if parsed_df is not None:
                    self.logger.info(f"‚úÖ Dados parseados com sucesso: {len(parsed_df)} linhas, {len(parsed_df.columns)} colunas originais")
                    self.logger.info(f"üìä Colunas reconstru√≠das: {list(parsed_df.columns)}")
                    return parsed_df
                else:
                    self.logger.warning("‚ö†Ô∏è Falha ao parsear chunk_text, retornando dados brutos da tabela embeddings")
            
            # Fallback: Remover colunas com tipos n√£o-hashable (metadata, embedding) para evitar erros
            if 'metadata' in df.columns:
                df = df.drop(columns=['metadata'])
            if 'embedding' in df.columns:
                df = df.drop(columns=['embedding'])
            
            return df
            
        except Exception as e:
            self.logger.error(f"Erro ao recuperar dados da tabela embeddings: {str(e)}")
            return None
    
    def _parse_chunk_text_to_dataframe(self, embeddings_df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """Parseia o conte√∫do CSV dentro do chunk_text para reconstruir DataFrame original.
        
        Args:
            embeddings_df: DataFrame com coluna chunk_text contendo CSV
            
        Returns:
            DataFrame com colunas originais do CSV ou None se falhar
        """
        try:
            all_rows = []
            header_found = None
            
            for idx, row in embeddings_df.iterrows():
                chunk_text = row.get('chunk_text', '')
                if not chunk_text or not isinstance(chunk_text, str):
                    continue
                
                # Separar linhas do chunk
                lines = chunk_text.strip().split('\n')
                
                # Procurar pelo cabe√ßalho CSV (linha que come√ßa com aspas ou tem muitas v√≠rgulas)
                for line_idx, line in enumerate(lines):
                    line = line.strip()
                    
                    # Pular linhas vazias, metadados ou descri√ß√µes
                    if not line or line.startswith('#') or line.startswith('CHUNK'):
                        continue
                    
                    # IMPORTANTE: Ignorar linha descritiva que come√ßa com "Colunas: "
                    # Exemplo: Colunas: "Time","V1","V2"... (isso √© metadado, n√£o o header real)
                    if line.startswith('Colunas:'):
                        continue
                    
                    # Pular linha separadora
                    if line.startswith('==='):
                        continue
                    
                    # Detectar header REAL: linha que come√ßa diretamente com aspas
                    # Header v√°lido para QUALQUER CSV: linha que come√ßa com " e cont√©m v√≠rgulas separando colunas
                    # Exemplos v√°lidos: "Time","V1","V2" ou "Nome","Idade","Cidade" ou "id","valor","status"
                    # A linha DEVE come√ßar com " (aspas) para ser considerada header v√°lido
                    if header_found is None and line.startswith('"') and '","' in line:
                        # Para ser header v√°lido, deve ter pelo menos 2 colunas (separadas por ",")
                        # Isso funciona para QUALQUER CSV, n√£o apenas creditcard
                        tentative_header = [col.strip().strip('"').strip() for col in line.split(',')]
                        tentative_header = [col for col in tentative_header if col]  # Remover vazios
                        
                        # Validar que temos pelo menos 2 colunas com nomes v√°lidos
                        if len(tentative_header) >= 2:
                            # Validar que os nomes n√£o s√£o apenas n√∫meros (provavelmente s√£o dados, n√£o header)
                            non_numeric_count = sum(1 for col in tentative_header[:5] if not col.replace('.','',1).replace('-','',1).isdigit())
                            
                            # Se a maioria das primeiras colunas n√£o s√£o puramente num√©ricas, √© um header v√°lido
                            if non_numeric_count >= max(2, len(tentative_header[:5]) // 2):
                                header_found = tentative_header
                                self.logger.info(f"üìã Header CSV detectado: {len(header_found)} colunas - {header_found[:5]}...")
                                continue
                    
                    # Se j√° temos header, parsear linhas de dados
                    # Linha de dados: n√£o come√ßa com aspas, tem v√≠rgulas, n√£o √© metadado
                    if header_found and ',' in line:
                        # Pular linhas de metadados/descri√ß√£o
                        skip_keywords = ['Chunk', 'Dataset', 'Cont√©m', 'Inclui', 'Features', 
                                       'Exemplo', 'Colunas:', 'Transa√ß√µes', '===', '---']
                        if any(line.startswith(kw) for kw in skip_keywords):
                            continue
                        
                        # Pular se for linha de header duplicada (come√ßa com aspas)
                        if line.startswith('"'):
                            continue
                        
                        try:
                            # Dividir por v√≠rgula
                            values = line.split(',')
                            
                            # Limpar valores (remover aspas extras se houver)
                            values = [v.strip().strip('"') for v in values]
                            
                            # Verificar se tem o mesmo n√∫mero de colunas do header
                            if len(values) == len(header_found):
                                all_rows.append(values)
                            elif len(values) > len(header_found):
                                # Truncar valores extras
                                all_rows.append(values[:len(header_found)])
                            elif len(values) >= len(header_found) - 2:  # Toler√¢ncia de 2 colunas
                                # Preencher com None
                                all_rows.append(values + [None] * (len(header_found) - len(values)))
                        except Exception as e:
                            self.logger.debug(f"Erro ao parsear linha: {str(e)}")
                            continue
            
            if not header_found:
                self.logger.warning("Nenhum header CSV encontrado no chunk_text")
                return None
                
            if not all_rows:
                self.logger.warning("Nenhuma linha de dados CSV encontrada no chunk_text")
                self.logger.debug(f"Amostra de chunk_text analisado (primeiros 500 chars): {str(embeddings_df['chunk_text'].iloc[0])[:500] if len(embeddings_df) > 0 else 'N/A'}")
                return None
            
            self.logger.info(f"üìä Parseando CSV: {len(all_rows)} linhas encontradas, {len(header_found)} colunas detectadas")
            self.logger.info(f"üìã Colunas: {header_found}")
            
            # Criar DataFrame
            df = pd.DataFrame(all_rows, columns=header_found)
            
            # Tentar converter colunas num√©ricas
            for col in df.columns:
                try:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                except:
                    pass
            
            self.logger.info(f"‚úÖ DataFrame reconstru√≠do: {len(df)} linhas, {len(df.columns)} colunas")
            self.logger.info(f"üìä Tipos de dados: {df.dtypes.to_dict()}")
            return df
            
        except Exception as e:
            self.logger.error(f"Erro ao parsear chunk_text: {str(e)}")
            import traceback
            self.logger.debug(traceback.format_exc())
            return None
    
    def get_data_from_supabase(self, table: str = 'embeddings', limit: int = None) -> Optional[pd.DataFrame]:
        """Recupera dados do Supabase como DataFrame.
        
        ‚ö†Ô∏è CONFORMIDADE: Apenas tabela 'embeddings' para agentes de resposta.
        
        Args:
            table: Nome da tabela ('embeddings', 'chunks', etc.)
            limit: Limite de registros (None para todos)
            
        Returns:
            DataFrame com os dados ou None se falhar
        """
        # Validar conformidade para agentes de resposta
        if table != 'embeddings' and self.caller_agent not in ['ingestion_agent', 'test_system']:
            error_msg = (
                f"‚ö†Ô∏è VIOLA√á√ÉO DE CONFORMIDADE DETECTADA!\n"
                f"Agente '{self.caller_agent}' tentou acessar tabela '{table}' diretamente.\n"
                f"Agentes de resposta devem usar APENAS a tabela 'embeddings'."
            )
            self.logger.error(error_msg)
            raise UnauthorizedCSVAccessError(error_msg)
        
        if not SUPABASE_CLIENT_AVAILABLE or not supabase:
            self.logger.error("Cliente Supabase n√£o dispon√≠vel")
            return None
        
        try:
            self.logger.info(f"Recuperando dados da tabela {table}...")
            
            query = supabase.table(table).select('*')
            if limit:
                query = query.limit(limit)
            
            result = query.execute()
            
            if not result.data:
                self.logger.warning(f"Nenhum dado encontrado na tabela {table}")
                return None
            
            df = pd.DataFrame(result.data)
            self.logger.info(f"Dados recuperados: {len(df)} registros, {len(df.columns)} colunas")
            return df
            
        except Exception as e:
            self.logger.error(f"Erro ao recuperar dados do Supabase: {str(e)}")
            return None
    
    def reconstruct_original_data(self) -> Optional[pd.DataFrame]:
        """Reconstr√≥i dados originais APENAS da tabela embeddings do Supabase.
        
        Returns:
            DataFrame com dados parseados do chunk_text ou None se falhar
            
        ‚ö†Ô∏è CONFORMIDADE TOTAL: APENAS SUPABASE EMBEDDINGS - NENHUM CSV.
        """
        try:
            self.logger.info("üîÑ Reconstruindo dados originais APENAS da tabela embeddings...")
            
            # √öNICA FONTE DE DADOS: Tabela embeddings do Supabase
            # O m√©todo get_data_from_embeddings() j√° parseia chunk_text automaticamente
            df = self.get_data_from_embeddings(limit=None, parse_chunk_text=True)
            
            if df is not None:
                self.logger.info(f"‚úÖ Dados reconstru√≠dos: {len(df)} registros, {len(df.columns)} colunas (CONFORMIDADE TOTAL)")
                return df
            
            # Se n√£o h√° dados, retornar None - NUNCA ler CSV
            self.logger.error(
                f"‚ùå Nenhum dado encontrado na tabela embeddings do Supabase.\n"
                f"‚ö†Ô∏è Sem fallback para CSV - APENAS embeddings permitido.\n"
                f"Execute a ingest√£o de dados primeiro: python ingest_completo.py"
            )
            return None
            
        except Exception as e:
            self.logger.error(f"Erro ao reconstruir dados originais: {str(e)}")
            return None
            
    def _detect_most_recent_csv(self) -> Optional[pd.DataFrame]:
        """Retorna dados APENAS da tabela embeddings (CONFORMIDADE TOTAL).
        
        ‚ö†Ô∏è NENHUM FALLBACK PARA CSV - APENAS SUPABASE EMBEDDINGS.
        """
        # APENAS dados da tabela embeddings - SEM EXCE√á√ïES
        embeddings_data = self.get_data_from_embeddings()
        if embeddings_data is not None:
            self.logger.info("‚úÖ Dados recuperados da tabela embeddings (CONFORMIDADE TOTAL)")
            return embeddings_data
        
        # Se n√£o h√° dados em embeddings, retornar None - NUNCA ler CSV
        self.logger.error(
            f"‚ùå Nenhum dado encontrado na tabela embeddings do Supabase.\n"
            f"‚ö†Ô∏è Sem fallback para CSV - APENAS embeddings permitido.\n"
            f"Execute a ingest√£o de dados primeiro: python ingest_completo.py"
        )
        return None
    
    def _reconstruct_csv_data(self, csv_filename: str) -> Optional[pd.DataFrame]:
        """Reconstr√≥i dados APENAS via embeddings do Supabase (CONFORMIDADE TOTAL).
        
        Args:
            csv_filename: Nome do arquivo CSV (ex: 'creditcard.csv', 'sales.csv') - APENAS REFER√äNCIA
            
        Returns:
            DataFrame com dados da tabela embeddings ou None se falhar
            
        ‚ö†Ô∏è NENHUM FALLBACK PARA CSV - APENAS SUPABASE EMBEDDINGS.
        """
        # APENAS dados da tabela embeddings - SEM EXCE√á√ïES - SEM FALLBACK
        embeddings_data = self.get_data_from_embeddings()
        if embeddings_data is not None:
            self.logger.info(f"‚úÖ Dados de {csv_filename} recuperados via embeddings (CONFORMIDADE TOTAL)")
            return embeddings_data
        
        # Se n√£o h√° dados em embeddings, retornar None - NUNCA ler CSV
        self.logger.error(
            f"‚ùå Nenhum dado encontrado na tabela embeddings para {csv_filename}.\n"
            f"‚ö†Ô∏è Sem fallback para CSV - APENAS embeddings permitido.\n"
            f"Execute a ingest√£o de dados primeiro: python ingest_completo.py"
        )
        return None
    
    def calculate_real_statistics(self, query_type: str = "tipos_dados") -> Dict[str, Any]:
        """Calcula estat√≠sticas reais dos dados usando Python.
        
        Args:
            query_type: Tipo de an√°lise ('tipos_dados', 'estatisticas', 'distribuicao')
            
        Returns:
            Dicion√°rio com estat√≠sticas calculadas
        """
        try:
            # Obter dados reais
            df = self.reconstruct_original_data()
            if df is None:
                return {"error": "N√£o foi poss√≠vel acessar dados reais"}
            
            self.logger.info(f"Calculando estat√≠sticas reais para: {query_type}")
            
            # SISTEMA GEN√âRICO: Analisar qualquer dataset
            result = {
                "data_source": "dataset gen√©rico",
                "total_records": len(df),
                "total_columns": len(df.columns),
                "columns": list(df.columns)
            }
            
            if query_type in ["tipos_dados", "all"]:
                # An√°lise gen√©rica de tipos de dados
                numeric_cols = []
                categorical_cols = []
                datetime_cols = []
                
                for col in df.columns:
                    try:
                        col_dtype = df[col].dtype
                        
                        if col_dtype in ['int64', 'float64', 'int32', 'float32', 'int8', 'int16', 'float16']:
                            numeric_cols.append(col)
                        elif col_dtype == 'object':
                            # Verificar se √© categ√≥rico real (strings/texto) ou se deveria ser num√©rico
                            try:
                                # Tentar converter para num√©rico para detectar n√∫meros como strings
                                pd.to_numeric(df[col].dropna().head(100))
                                # Se conseguiu converter, pode ser num√©rico mal formatado
                                numeric_cols.append(col)
                            except (ValueError, TypeError):
                                # Verificar se √© categ√≥rico (poucos valores √∫nicos) ou texto
                                categorical_cols.append(col)
                        elif 'datetime' in str(col_dtype).lower():
                            datetime_cols.append(col)
                        else:
                            # Para tipos desconhecidos, tentar detectar se √© num√©rico
                            try:
                                if col_dtype.kind in 'biufc':  # boolean, int, unsigned, float, complex
                                    numeric_cols.append(col)
                                else:
                                    categorical_cols.append(col)
                            except:
                                categorical_cols.append(col)
                    except Exception as e:
                        self.logger.warning(f"Erro ao analisar dtype da coluna '{col}': {str(e)}")
                        categorical_cols.append(col)
                
                result.update({
                    "tipos_dados": {
                        "numericos": numeric_cols,
                        "categoricos": categorical_cols,
                        "datetime": datetime_cols,
                        "total_numericos": len(numeric_cols),
                        "total_categoricos": len(categorical_cols),
                        "total_datetime": len(datetime_cols)
                    }
                })
            
            if query_type in ["estatisticas", "all"]:
                # Estat√≠sticas gen√©ricas para colunas num√©ricas
                estatisticas = {}
                
                for col in df.select_dtypes(include=['number']).columns:
                    estatisticas[col] = {
                        "tipo": str(df[col].dtype),
                        "count": int(df[col].count()),
                        "mean": float(df[col].mean()),
                        "std": float(df[col].std()),
                        "min": float(df[col].min()),
                        "max": float(df[col].max()),
                        "median": float(df[col].median()),
                        "q25": float(df[col].quantile(0.25)),
                        "q75": float(df[col].quantile(0.75))
                    }
                
                # Estat√≠sticas para colunas categ√≥ricas
                for col in df.select_dtypes(include=['object']).columns:
                    try:
                        value_counts = df[col].value_counts()
                        if len(value_counts) <= 20:  # S√≥ mostrar se n√£o h√° muitas categorias
                            estatisticas[col] = {
                                "tipo": str(df[col].dtype),
                                "unique_values": df[col].unique().tolist()[:10],  # M√°ximo 10 valores
                                "value_counts": value_counts.head(10).to_dict(),
                                "percentages": (value_counts.head(10) / len(df) * 100).round(2).to_dict()
                            }
                    except Exception as e:
                        self.logger.warning(f"Erro ao calcular estat√≠sticas para coluna categ√≥rica '{col}': {str(e)}")
                        continue
                
                result.update({"estatisticas": estatisticas})
            
            if query_type in ["distribuicao", "all"]:
                # Distribui√ß√µes gen√©ricas
                distribuicao = {}
                
                # Para cada coluna categ√≥rica, calcular distribui√ß√£o
                for col in df.select_dtypes(include=['object']).columns:
                    value_counts = df[col].value_counts()
                    if len(value_counts) <= 10:  # S√≥ para colunas com poucas categorias
                        dist_dict = {}
                        for value, count in value_counts.items():
                            percentage = count / len(df) * 100
                            dist_dict[str(value)] = {
                                "count": int(count),
                                "percentage": float(percentage)
                            }
                        distribuicao[col] = dist_dict
                
                # Estat√≠sticas de range para colunas num√©ricas
                numeric_ranges = {}
                for col in df.select_dtypes(include=['number']).columns:
                    numeric_ranges[col] = {
                        "range_min": float(df[col].min()),
                        "range_max": float(df[col].max()),
                        "range_amplitude": float(df[col].max() - df[col].min())
                    }
                
                result.update({
                    "distribuicao": distribuicao,
                    "ranges_numericos": numeric_ranges
                })
            
            return result
            
        except Exception as e:
            error_msg = f"Erro ao calcular estat√≠sticas: {str(e)}"
            self.logger.error(error_msg)
            return {"error": error_msg}
    
    def execute_safe_python(self, code: str, context: Dict[str, Any] = None) -> PythonAnalysisResult:
        """Executa c√≥digo Python de forma segura.
        
        Args:
            code: C√≥digo Python para executar
            context: Contexto adicional (DataFrames, vari√°veis)
            
        Returns:
            Resultado da execu√ß√£o
        """
        import time
        start_time = time.time()
        
        # Capturar output
        old_stdout = sys.stdout
        sys.stdout = captured_output = io.StringIO()
        
        try:
            # Preparar namespace seguro
            local_vars = self.safe_globals.copy()
            if context:
                local_vars.update(context)
            
            # Executar c√≥digo
            exec(code, {"__builtins__": {}}, local_vars)
            
            # Capturar resultado
            output = captured_output.getvalue()
            execution_time = time.time() - start_time
            
            return PythonAnalysisResult(
                success=True,
                result=local_vars.get('result'),
                output=output,
                execution_time=execution_time
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"Erro na execu√ß√£o: {str(e)}\n{traceback.format_exc()}"
            
            return PythonAnalysisResult(
                success=False,
                result=None,
                output=captured_output.getvalue(),
                error=error_msg,
                execution_time=execution_time
            )
        
        finally:
            sys.stdout = old_stdout
    
    def calculate_clustering_analysis(self, n_clusters: int = 3) -> Dict[str, Any]:
        """
        Calcula an√°lise de clustering (KMeans) nos dados num√©ricos do dataset.
        
        Args:
            n_clusters: N√∫mero de clusters desejados (padr√£o: 3)
            
        Returns:
            Dicion√°rio com informa√ß√µes sobre os clusters encontrados
        """
        try:
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler
            
            # 1. Recuperar dados do Supabase usando reconstruct_original_data
            df = self.reconstruct_original_data()
            
            if df is None or df.empty:
                return {
                    "error": "Nenhum dado dispon√≠vel para an√°lise de clustering",
                    "suggestion": "Certifique-se de que os dados foram ingeridos na tabela embeddings"
                }
            
            # 2. Selecionar apenas colunas num√©ricas
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # Remover colunas irrelevantes como ID, √≠ndice, etc.
            numeric_cols = [col for col in numeric_cols if col.lower() not in ['id', 'index', 'unnamed: 0']]
            
            if len(numeric_cols) == 0:
                return {
                    "error": "Nenhuma coluna num√©rica encontrada para clustering",
                    "available_columns": df.columns.tolist()
                }
            
            self.logger.info(f"üî¨ Aplicando KMeans com {n_clusters} clusters em {len(numeric_cols)} vari√°veis num√©ricas")
            
            # 3. Preparar dados para clustering
            X = df[numeric_cols].copy()
            
            # Remover linhas com valores nulos
            X = X.dropna()
            
            if len(X) == 0:
                return {
                    "error": "Todos os dados t√™m valores nulos nas colunas num√©ricas"
                }
            
            # 4. Normalizar dados (importante para KMeans)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # 5. Aplicar KMeans
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_scaled)
            
            # 6. Contar pontos por cluster
            unique, counts = np.unique(cluster_labels, return_counts=True)
            cluster_distribution = dict(zip(unique.tolist(), counts.tolist()))
            
            # 7. Calcular percentuais
            total_points = len(cluster_labels)
            cluster_percentages = {
                cluster: (count / total_points) * 100 
                for cluster, count in cluster_distribution.items()
            }
            
            # 8. Calcular centr√≥ides (caracter√≠sticas m√©dias de cada cluster)
            centroids = kmeans.cluster_centers_
            
            # 9. Verificar balanceamento
            max_cluster_pct = max(cluster_percentages.values())
            min_cluster_pct = min(cluster_percentages.values())
            is_balanced = (max_cluster_pct / min_cluster_pct) < 3.0  # threshold arbitr√°rio
            
            # 10. Construir resposta estruturada
            result = {
                "success": True,
                "n_clusters": n_clusters,
                "total_points": total_points,
                "numeric_variables_used": numeric_cols,
                "cluster_distribution": cluster_distribution,
                "cluster_percentages": cluster_percentages,
                "is_balanced": is_balanced,
                "balance_ratio": max_cluster_pct / min_cluster_pct if min_cluster_pct > 0 else float('inf'),
                "inertia": float(kmeans.inertia_),  # Soma das dist√¢ncias quadradas aos centr√≥ides
                "interpretation": self._interpret_clustering_results(
                    cluster_distribution,
                    cluster_percentages,
                    is_balanced,
                    numeric_cols
                )
            }
            
            self.logger.info(f"‚úÖ Clustering conclu√≠do: {cluster_distribution}")
            return result
            
        except ImportError:
            return {
                "error": "Biblioteca scikit-learn n√£o est√° instalada",
                "suggestion": "Execute: pip install scikit-learn"
            }
        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise de clustering: {str(e)}", exc_info=True)
            return {
                "error": f"Erro ao calcular clustering: {str(e)}",
                "traceback": traceback.format_exc()
            }
    
    def _interpret_clustering_results(
        self,
        cluster_distribution: Dict[int, int],
        cluster_percentages: Dict[int, float],
        is_balanced: bool,
        numeric_cols: List[str]
    ) -> str:
        """Gera interpreta√ß√£o textual dos resultados de clustering."""
        
        interpretation = "**An√°lise de Clustering (KMeans):**\n\n"
        
        # Distribui√ß√£o dos clusters
        interpretation += f"Os dados foram agrupados em {len(cluster_distribution)} clusters distintos:\n\n"
        
        for cluster_id in sorted(cluster_distribution.keys()):
            count = cluster_distribution[cluster_id]
            pct = cluster_percentages[cluster_id]
            interpretation += f"- **Cluster {cluster_id}:** {count:,} pontos ({pct:.1f}%)\n"
        
        interpretation += "\n"
        
        # Avalia√ß√£o do balanceamento
        if is_balanced:
            interpretation += "‚úÖ **Balanceamento:** Os clusters est√£o relativamente balanceados, "
            interpretation += "indicando grupos de tamanhos similares nos dados.\n\n"
        else:
            interpretation += "‚ö†Ô∏è **Balanceamento:** Os clusters est√£o desbalanceados, "
            interpretation += "sugerindo que h√° um grupo dominante e outros menores.\n\n"
        
        # Vari√°veis utilizadas
        if len(numeric_cols) > 5:
            interpretation += f"**Vari√°veis Utilizadas:** {len(numeric_cols)} vari√°veis num√©ricas "
            interpretation += f"(incluindo {', '.join(numeric_cols[:3])}, ...)\n\n"
        else:
            interpretation += f"**Vari√°veis Utilizadas:** {', '.join(numeric_cols)}\n\n"
        
        # Conclus√£o
        interpretation += "**Conclus√£o:** SIM, os dados apresentam estrutura de agrupamentos naturais. "
        interpretation += "Diferentes t√©cnicas de clustering ou n√∫mero de clusters podem revelar padr√µes adicionais."
        
        return interpretation

# Inst√¢ncia global para uso pelos agentes
python_analyzer = PythonDataAnalyzer()