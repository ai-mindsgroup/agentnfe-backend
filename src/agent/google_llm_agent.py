"""Agente LLM usando Google Generative AI (Gemini)
===============================================

Este agente utiliza o Google Gemini para an√°lises inteligentes e insights.
Integra com o sistema multiagente para fornecer capacidades avan√ßadas de NLP.
"""

from __future__ import annotations
import sys
import os
from pathlib import Path

# Adiciona o diret√≥rio raiz do projeto ao PYTHONPATH
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

from typing import Dict, Any, Optional, List
import json
from dataclasses import dataclass

from src.agent.base_agent import BaseAgent, AgentError
from src.settings import GOOGLE_API_KEY
from src.utils.logging_config import get_logger

# Import condicional do sistema RAG
try:
    from src.embeddings.vector_store import VectorStore
    from src.embeddings.generator import EmbeddingGenerator, EmbeddingProvider
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    VectorStore = None
    EmbeddingGenerator = None

# Import condicional do Google Generative AI
try:
    import google.generativeai as genai
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    GOOGLE_AI_AVAILABLE = False
    genai = None

logger = get_logger(__name__)


@dataclass
class LLMRequest:
    """Request para o LLM."""
    prompt: str
    context: Optional[Dict[str, Any]] = None
    temperature: float = 0.3
    max_tokens: int = 1000
    system_prompt: Optional[str] = None


@dataclass 
class LLMResponse:
    """Resposta do LLM."""
    content: str
    usage: Dict[str, Any]
    model: str
    success: bool = True
    error: Optional[str] = None


class GoogleLLMAgent(BaseAgent):
    """Agente que utiliza Google Generative AI (Gemini) para an√°lises inteligentes."""
    
    def __init__(self, model: str = "gemini-2.0-flash"):
        super().__init__(
            name="google_llm",
            description="Agente LLM usando Google Gemini para an√°lises inteligentes e insights"
        )
        
        self.model_name = model
        self.model = None
        
        # Verificar disponibilidade
        if not GOOGLE_AI_AVAILABLE:
            raise AgentError(
                self.name,
                "Google Generative AI n√£o dispon√≠vel. Execute: pip install google-generativeai"
            )
        
        if not GOOGLE_API_KEY:
            raise AgentError(
                self.name, 
                "GOOGLE_API_KEY n√£o configurado. Configure em configs/.env"
            )
        
        # Inicializar sistema RAG se dispon√≠vel
        self.rag_enabled = False
        self.vector_store = None
        self.embedding_generator = None
        
        if RAG_AVAILABLE:
            try:
                self.vector_store = VectorStore()
                self.embedding_generator = EmbeddingGenerator(EmbeddingProvider.SENTENCE_TRANSFORMER)
                self.rag_enabled = True
                self.logger.info("RAG integrado ao Google LLM Agent")
            except Exception as e:
                self.logger.warning(f"RAG n√£o dispon√≠vel: {e}")
        
        # Configurar e inicializar Google Gemini
        try:
            genai.configure(api_key=GOOGLE_API_KEY)
            self.model = genai.GenerativeModel(model)
            self.logger.info(f"Google LLM inicializado: {model}")
            
        except Exception as e:
            raise AgentError(self.name, f"Erro na inicializa√ß√£o do Google LLM: {e}")
    
    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Processa consulta usando busca RAG + Google Gemini (sistema h√≠brido).
        
        Fluxo:
        1. Se RAG dispon√≠vel, busca consultas similares no banco vetorial
        2. Se encontrar resposta relevante, usa ela (cache inteligente)  
        3. Se n√£o encontrar, chama Google Gemini
        4. Salva consulta + resposta LLM no banco vetorial
        
        Args:
            query: Consulta/prompt para o LLM
            context: Contexto adicional (dados, configura√ß√µes, etc.)
            
        Returns:
            Resposta estruturada do LLM ou cache
        """
        try:
            # 1. BUSCAR NO CACHE VETORIAL (se dispon√≠vel)
            if self.rag_enabled:
                cached_response = self._search_cached_response(query)
                if cached_response:
                    self.logger.info("üíæ Usando resposta em cache (RAG)")
                    return self._build_response(
                        cached_response["content"],
                        metadata={
                            "model": "cache_rag", 
                            "llm_used": False,
                            "cache_used": True,
                            "similarity_score": cached_response.get("similarity", 0.0),
                            "success": True
                        }
                    )
            
            # 2. GERAR NOVA RESPOSTA COM LLM
            self.logger.info("ü§ñ Gerando nova resposta com Google Gemini")
            
            # Preparar request
            llm_request = self._prepare_request(query, context)
            
            # Enviar para Google Gemini
            response = self._call_gemini(llm_request)
            
            # 3. SALVAR NO BANCO VETORIAL (se dispon√≠vel)
            if self.rag_enabled and response.success:
                self._save_to_vector_store(query, response.content, context)
            
            # Processar resposta
            return self._build_response(
                response.content,
                metadata={
                    "model": response.model,
                    "usage": response.usage,
                    "llm_used": True,
                    "cache_used": False,
                    "success": response.success
                }
            )
            
        except Exception as e:
            self.logger.error(f"Erro no processamento LLM h√≠brido: {e}")
            return self._build_response(
                f"Erro no processamento: {str(e)}",
                metadata={"error": True, "llm_used": False, "cache_used": False}
            )
    
    def _prepare_request(self, query: str, context: Optional[Dict[str, Any]]) -> LLMRequest:
        """Prepara request para o LLM com contexto e system prompt."""
        
        # System prompt para an√°lise de dados
        system_prompt = """Voc√™ √© um especialista em an√°lise de dados e detec√ß√£o de fraudes.
        
Suas responsabilidades:
- Analisar dados CSV e identificar padr√µes
- Detectar anomalias e poss√≠veis fraudes
- Fornecer insights estrat√©gicos baseados em dados
- Explicar correla√ß√µes e tend√™ncias
- Sugerir a√ß√µes para melhorar seguran√ßa

Diretrizes:
- Seja preciso e baseie-se nos dados fornecidos
- Use linguagem t√©cnica mas acess√≠vel
- Destaque descobertas importantes
- Forne√ßa recomenda√ß√µes pr√°ticas
- Seja conciso mas completo
"""
        
        # Construir prompt com contexto
        if context:
            prompt_parts = [system_prompt, "\nContexto dos dados:"]
            
            # Adicionar informa√ß√µes do contexto
            if "file_path" in context:
                prompt_parts.append(f"Arquivo: {context['file_path']}")
            
            if "data_info" in context:
                data_info = context["data_info"]
                prompt_parts.append(f"Dimens√µes: {data_info.get('rows', 'N/A')} linhas √ó {data_info.get('columns', 'N/A')} colunas")
            
            if "fraud_data" in context:
                fraud_info = context["fraud_data"] 
                prompt_parts.append(f"Fraudes: {fraud_info.get('count', 0)} de {fraud_info.get('total', 0)} transa√ß√µes")
            
            # Adicionar consulta do usu√°rio
            prompt_parts.extend(["\nConsulta do usu√°rio:", query])
            
            full_prompt = "\n".join(prompt_parts)
        else:
            full_prompt = f"{system_prompt}\n\nConsulta: {query}"
        
        return LLMRequest(
            prompt=full_prompt,
            context=context,
            temperature=0.3,  # Mais determin√≠stico para an√°lise de dados
            max_tokens=1000
        )
    
    def _call_gemini(self, request: LLMRequest) -> LLMResponse:
        """Chama Google Gemini API."""
        try:
            # Configurar par√¢metros de gera√ß√£o
            generation_config = {
                "temperature": request.temperature,
                "max_output_tokens": request.max_tokens,
                "candidate_count": 1,
            }
            
            # Gerar resposta
            response = self.model.generate_content(
                request.prompt,
                generation_config=generation_config
            )
            
            # Extrair conte√∫do
            content = response.text if response.text else "Sem resposta gerada"
            
            # Metadados de uso (Gemini n√£o fornece tokens detalhados via API gratuita)
            usage = {
                "prompt_tokens": len(request.prompt.split()),  # Aproximado
                "completion_tokens": len(content.split()),     # Aproximado  
                "total_tokens": len(request.prompt.split()) + len(content.split())
            }
            
            return LLMResponse(
                content=content,
                usage=usage,
                model=self.model_name,
                success=True
            )
            
        except Exception as e:
            self.logger.error(f"Erro na chamada Gemini: {e}")
            return LLMResponse(
                content=f"Erro na gera√ß√£o: {str(e)}",
                usage={},
                model=self.model_name,
                success=False,
                error=str(e)
            )
    
    def analyze_data_insights(self, data_summary: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa dados e gera insights inteligentes."""
        
        # Construir prompt especializado para an√°lise de dados
        prompt = f"""Analise os seguintes dados e forne√ßa insights estrat√©gicos:

RESUMO DOS DADOS:
{json.dumps(data_summary, indent=2, ensure_ascii=False)}

Por favor, forne√ßa:
1. **Principais Descobertas**: 3-5 insights mais importantes
2. **Padr√µes Identificados**: Tend√™ncias ou correla√ß√µes relevantes  
3. **Riscos Potenciais**: Poss√≠veis problemas ou anomalias
4. **Recomenda√ß√µes**: 3-5 a√ß√µes estrat√©gicas espec√≠ficas
5. **Pr√≥ximos Passos**: Sugest√µes para an√°lises adicionais

Formato: Use markdown para estruturar a resposta de forma clara."""
        
        context = {"analysis_type": "data_insights", "data_summary": data_summary}
        
        return self.process(prompt, context)
    
    def detect_fraud_patterns(self, fraud_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa padr√µes de fraude e fornece insights especializados."""
        
        prompt = f"""Analise os padr√µes de fraude nos seguintes dados:

DADOS DE FRAUDE:
{json.dumps(fraud_data, indent=2, ensure_ascii=False)}

Forne√ßa uma an√°lise especializada incluindo:

1. **An√°lise de Risco**: Avalie o n√≠vel de risco atual
2. **Padr√µes de Fraude**: Identifique caracter√≠sticas comuns das fraudes
3. **Fatores de Risco**: Quais vari√°veis mais indicam fraude?
4. **Estrat√©gias de Preven√ß√£o**: Como reduzir fraudes futuras?
5. **Alertas Cr√≠ticos**: Situa√ß√µes que requerem aten√ß√£o imediata

Use dados concretos e seja espec√≠fico nas recomenda√ß√µes."""
        
        context = {"analysis_type": "fraud_detection", "fraud_data": fraud_data}
        
        return self.process(prompt, context)
    
    def explain_correlations(self, correlation_data: Dict[str, Any]) -> Dict[str, Any]:
        """Explica correla√ß√µes de forma acess√≠vel para neg√≥cios."""
        
        prompt = f"""Explique as correla√ß√µes nos dados de forma acess√≠vel para stakeholders de neg√≥cio:

CORRELA√á√ïES ENCONTRADAS:
{json.dumps(correlation_data, indent=2, ensure_ascii=False)}

Explique:
1. **O que significam** essas correla√ß√µes em termos pr√°ticos
2. **Implica√ß√µes de neg√≥cio** de cada correla√ß√£o importante
3. **Oportunidades** identificadas atrav√©s das correla√ß√µes
4. **Riscos** potenciais revelados pelos dados
5. **A√ß√µes recomendadas** baseadas nessas descobertas

Use linguagem acess√≠vel e evite jarg√£o estat√≠stico excessivo."""
        
        context = {"analysis_type": "correlation_analysis", "correlation_data": correlation_data}
        
        return self.process(prompt, context)

    def _search_cached_response(self, query: str, similarity_threshold: float = 0.8) -> Optional[Dict[str, Any]]:
        """Busca respostas similares no cache vetorial.
        
        Args:
            query: Consulta para buscar
            similarity_threshold: Limite m√≠nimo de similaridade (0-1)
            
        Returns:
            Resposta em cache se encontrada, sen√£o None
        """
        if not self.rag_enabled:
            return None
            
        try:
            # Gerar embedding da consulta
            query_result = self.embedding_generator.generate_embedding(query)
            query_embedding = query_result.embedding  # Extrair lista de floats
            
            # Buscar consultas similares
            results = self.vector_store.search_similar(
                query_embedding=query_embedding,
                limit=1,
                similarity_threshold=similarity_threshold
            )
            
            if results and len(results) > 0:
                best_match = results[0]
                self.logger.info(f"üéØ Cache hit! Similaridade: {best_match.similarity_score:.3f}")
                
                return {
                    "content": best_match.metadata.get("response", ""),
                    "similarity": best_match.similarity_score,
                    "original_query": best_match.chunk_text
                }
                
        except Exception as e:
            self.logger.warning(f"Erro na busca de cache: {e}")
            
        return None
    
    def _save_to_vector_store(self, query: str, response: str, context: Optional[Dict[str, Any]] = None) -> bool:
        """Salva consulta e resposta no banco vetorial para cache futuro com metadata enriquecido.
        
        Args:
            query: Consulta original
            response: Resposta do LLM
            context: Contexto adicional
            
        Returns:
            True se salvou com sucesso, False caso contr√°rio
        """
        if not self.rag_enabled:
            return False
            
        try:
            from datetime import datetime
            
            # Preparar metadados enriquecidos para busca contextual
            metadata = {
                "agent": self.name,
                "model": self.model_name,
                "response_content": response[:500],  # Truncar resposta longa
                "timestamp": datetime.now().isoformat(),
                "query_type": "llm_analysis",
                "embedding_type": "conversation",
                "session_id": getattr(self, '_current_session_id', None),
                "context_keys": list(context.keys()) if context else [],
            }
            
            # Adicionar contexto relevante se dispon√≠vel
            if context:
                if "file_path" in context:
                    metadata["source_file"] = context["file_path"]
                if "data_info" in context:
                    metadata["data_dimensions"] = f"{context['data_info'].get('rows', 0)}x{context['data_info'].get('columns', 0)}"
                if "fraud_data" in context:
                    metadata["fraud_count"] = context["fraud_data"].get("count", 0)
            
            # Gerar e salvar embedding
            query_result = self.embedding_generator.generate_embedding(query)
            query_embedding = query_result.embedding  # Extrair lista de floats
            
            result = self.vector_store.store_embedding(
                query=query,
                response=response,
                embedding=query_embedding,
                metadata=metadata  # Passar metadata enriquecido
            )
            
            if result:
                self.logger.info(f"üíæ Consulta salva no cache vetorial com metadata enriquecido: {list(metadata.keys())}")
                return True
            else:
                self.logger.warning("Falha ao salvar no cache vetorial")
                return False
                
        except Exception as e:
            self.logger.error(f"Erro ao salvar no cache: {e}")
            return False