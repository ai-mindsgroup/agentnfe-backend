"""Agente Orquestrador Central para coordenar sistema multiagente.

Este agente √© respons√°vel por:
- Receber consultas dos usu√°rios
- Determinar qual(is) agente(s) especializado(s) utilizar
- Coordenar m√∫ltiplos agentes quando necess√°rio
- Combinar respostas de diferentes agentes
- Manter contexto da conversa√ß√£o
- Fornecer interface √∫nica para o sistema completo
"""
from __future__ import annotations
import sys
import os
from pathlib import Path

# Adiciona o diret√≥rio raiz do projeto ao PYTHONPATH
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

import re
from typing import Any, Dict, List, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum

from src.agent.base_agent import BaseAgent, AgentError
from src.agent.rag_data_agent import RAGDataAgent  # Agente RAG puro sem keywords hardcoded
from src.data.data_processor import DataProcessor
from src.memory.memory_types import ContextType
from src.settings import (
    SEMANTIC_MEMORY_RECALL_LIMIT,
    SEMANTIC_MEMORY_SIMILARITY_THRESHOLD,
)

# Import condicional do RAGAgent (pode falhar se Supabase n√£o configurado)
try:
    from src.agent.rag_agent import RAGAgent
    RAG_AGENT_AVAILABLE = True
except ImportError as e:
    RAG_AGENT_AVAILABLE = False
    RAGAgent = None
    print(f"[AVISO] RAGAgent n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    RAG_AGENT_AVAILABLE = False  
    RAGAgent = None
    print(f"[AVISO] RAGAgent n√£o dispon√≠vel: {str(e)[:100]}...")

# Import do cliente Supabase para verifica√ß√£o de dados
try:
    from src.vectorstore.supabase_client import supabase
    SUPABASE_CLIENT_AVAILABLE = True
except ImportError as e:
    SUPABASE_CLIENT_AVAILABLE = False
    supabase = None
    print(f"[AVISO] Cliente Supabase n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    SUPABASE_CLIENT_AVAILABLE = False  
    supabase = None
    print(f"[AVISO] Cliente Supabase n√£o dispon√≠vel: {str(e)[:100]}...")

# Import do agente especialista em NFe
try:
    from src.agent.nfe_tax_specialist_agent import NFeTaxSpecialistAgent
    NFE_AGENT_AVAILABLE = True
except ImportError as e:
    NFE_AGENT_AVAILABLE = False
    NFeTaxSpecialistAgent = None
    print(f"‚ö†Ô∏è NFeTaxSpecialistAgent n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    NFE_AGENT_AVAILABLE = False
    NFeTaxSpecialistAgent = None
    print(f"‚ö†Ô∏è NFeTaxSpecialistAgent n√£o dispon√≠vel: {str(e)[:100]}...")

# Import da ferramenta de an√°lise Python
try:
    from src.tools.python_analyzer import python_analyzer
    PYTHON_ANALYZER_AVAILABLE = True
except ImportError as e:
    PYTHON_ANALYZER_AVAILABLE = False
    python_analyzer = None
    print(f"‚ö†Ô∏è Python Analyzer n√£o dispon√≠vel: {str(e)[:100]}...")

# Import dos guardrails de valida√ß√£o
try:
    from src.tools.guardrails import statistics_guardrails
    GUARDRAILS_AVAILABLE = True
except ImportError as e:
    GUARDRAILS_AVAILABLE = False
    statistics_guardrails = None
    print(f"‚ö†Ô∏è Guardrails n√£o dispon√≠vel: {str(e)[:100]}...")# Import do LLM Manager (camada de abstra√ß√£o para m√∫ltiplos provedores)
try:
    from src.llm.manager import get_llm_manager, LLMManager, LLMConfig
    LLM_MANAGER_AVAILABLE = True
except ImportError as e:
    LLM_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è LLM Manager n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    LLM_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è LLM Manager n√£o dispon√≠vel: {str(e)[:100]}...")

# Import do sistema de prompts
try:
    from src.prompts.manager import get_prompt_manager, AgentRole
    PROMPT_MANAGER_AVAILABLE = True
except ImportError as e:
    PROMPT_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è Prompt Manager n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    PROMPT_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è Prompt Manager n√£o dispon√≠vel: {str(e)[:100]}...")

# Import do Roteador Sem√¢ntico para classifica√ß√£o inteligente de inten√ß√µes
try:
    from src.router.semantic_router import SemanticRouter
    SEMANTIC_ROUTER_AVAILABLE = True
except ImportError as e:
    SEMANTIC_ROUTER_AVAILABLE = False
    print(f"‚ö†Ô∏è Semantic Router n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    SEMANTIC_ROUTER_AVAILABLE = False
    print(f"‚ö†Ô∏è Semantic Router n√£o dispon√≠vel: {str(e)[:100]}...")


class QueryType(Enum):
    """Tipos de consultas que o orquestrador pode processar."""
    CSV_ANALYSIS = "csv_analysis"      # An√°lise de dados CSV gen√©ricos
    RAG_SEARCH = "rag_search"          # Busca sem√¢ntica/contextual
    DATA_LOADING = "data_loading"      # Carregamento de dados
    LLM_ANALYSIS = "llm_analysis"      # An√°lise via LLM (Google Gemini)
    HYBRID = "hybrid"                  # M√∫ltiplos agentes necess√°rios
    GENERAL = "general"                # Consulta geral/conversacional
    NFE_ANALYSIS = "nfe_analysis"      # An√°lise de Notas Fiscais Eletr√¥nicas
    UNKNOWN = "unknown"                # Tipo n√£o identificado


@dataclass
class AgentTask:
    """Representa uma tarefa para um agente espec√≠fico."""
    agent_name: str
    query: str
    context: Optional[Dict[str, Any]] = None
    priority: int = 1  # 1=alta, 2=m√©dia, 3=baixa


@dataclass
class OrchestratorResponse:
    """Resposta consolidada do orquestrador."""
    content: str
    query_type: QueryType
    agents_used: List[str]
    metadata: Dict[str, Any]
    success: bool = True
    error: Optional[str] = None


class OrchestratorAgent(BaseAgent):
    """Agente central que coordena todos os agentes especializados."""
    
    def __init__(self, 
                 enable_csv_agent: bool = True,
                 enable_rag_agent: bool = True,
                 enable_llm_manager: bool = True,
                 enable_data_processor: bool = True,
                 enable_nfe_agent: bool = True):
        """Inicializa o orquestrador com agentes especializados.
        
        Args:
            enable_csv_agent: Habilitar agente de an√°lise CSV
            enable_rag_agent: Habilitar agente RAG
            enable_llm_manager: Habilitar LLM Manager (camada de abstra√ß√£o para m√∫ltiplos LLMs)
            enable_data_processor: Habilitar processador de dados
            enable_nfe_agent: Habilitar agente especialista em NFe
        """
        super().__init__(
            name="orchestrator",
            description="Coordenador central do sistema multiagente de IA para an√°lise de dados",
            enable_memory=True  # Habilita sistema de mem√≥ria
        )
        
        # Inicializar agentes especializados
        self.agents = {}
        # Palavras-chave para detec√ß√£o de visualiza√ß√µes (usado para setar flags)
        self._viz_keywords = {
            'histogram': ['histograma', 'histogram', 'histograms', 'distribui√ß√£o', 'distribuicao', 'distribuicoes', 'distributions'],
            'bar': ['barras', 'bar', 'barplot', 'bar chart', 'gr√°fico', 'grafico'],
            'scatter': ['scatter', 'dispers√£o', 'dispersao', 'scatterplot'],
            'box': ['boxplot', 'box plot', 'box'],
        }
        
        # MIGRA√á√ÉO: conversation_history e current_data_context agora s√£o persistentes
        # Mant√©m compatibilidade tempor√°ria para transi√ß√£o gradual
        self.conversation_history = []  # DEPRECIADO - usar mem√≥ria Supabase
        self.current_data_context = {}  # DEPRECIADO - usar mem√≥ria Supabase
        
        # Inicializar LLM Manager (camada de abstra√ß√£o)
        self.llm_manager = None
        
        # Inicializar Prompt Manager
        self.prompt_manager = None
        if PROMPT_MANAGER_AVAILABLE:
            try:
                self.prompt_manager = get_prompt_manager()
                self.logger.info("[OK] Prompt Manager inicializado")
            except Exception as e:
                self.logger.warning(f"[AVISO] Falha ao inicializar Prompt Manager: {str(e)}")
        
        # Inicializar agentes com tratamento de erro gracioso
        initialization_errors = []
        
        # CSV Agent (sempre dispon√≠vel - sem depend√™ncias externas)
        # ATUALIZADO: Usa RAGDataAgent que implementa busca vetorial pura
        if enable_csv_agent:
            try:
                self.agents["csv"] = RAGDataAgent()
                self.logger.info("‚úÖ Agente RAG Data (CSV) inicializado - busca vetorial pura")
            except Exception as e:
                error_msg = f"RAG Data Agent: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        
        # RAG Agent (requer Supabase configurado)
        if enable_rag_agent and RAG_AGENT_AVAILABLE:
            try:
                self.agents["rag"] = RAGAgent()
                self.logger.info("‚úÖ Agente RAG inicializado")
            except Exception as e:
                error_msg = f"RAG Agent: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        elif enable_rag_agent and not RAG_AGENT_AVAILABLE:
            error_msg = "RAG Agent: Depend√™ncias n√£o dispon√≠veis (Supabase n√£o configurado)"
            initialization_errors.append(error_msg)
            self.logger.warning(f"‚ö†Ô∏è {error_msg}")

        # NFe Tax Specialist Agent (especialista em Notas Fiscais Eletr√¥nicas)
        if enable_nfe_agent and NFE_AGENT_AVAILABLE:
            try:
                self.agents["nfe"] = NFeTaxSpecialistAgent()
                self.logger.info("‚úÖ Agente NFe Tax Specialist inicializado")
            except Exception as e:
                error_msg = f"NFe Tax Specialist: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        elif enable_nfe_agent and not NFE_AGENT_AVAILABLE:
            error_msg = "NFe Tax Specialist: Depend√™ncias n√£o dispon√≠veis"
            initialization_errors.append(error_msg)
            self.logger.warning(f"‚ö†Ô∏è {error_msg}")

        # LLM Manager (camada de abstra√ß√£o para m√∫ltiplos provedores)
        if enable_llm_manager and LLM_MANAGER_AVAILABLE:
            try:
                self.llm_manager = get_llm_manager()
                self.logger.info("‚úÖ LLM Manager inicializado")
                
                # Adicionar informa√ß√µes do provedor ativo
                status = self.llm_manager.get_status()
                active_provider = status.get("active_provider", "unknown")
                self.logger.info(f"ü§ñ Provedor LLM ativo: {active_provider}")
                
            except Exception as e:
                error_msg = f"LLM Manager: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        elif enable_llm_manager and not LLM_MANAGER_AVAILABLE:
            error_msg = "LLM Manager: Depend√™ncias n√£o dispon√≠veis"
            initialization_errors.append(error_msg)
            self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        
        # Data Processor (sempre dispon√≠vel - sem depend√™ncias externas)  
        if enable_data_processor:
            try:
                self.data_processor = DataProcessor(caller_agent='orchestrator_agent')
                self.logger.info("‚úÖ Data Processor inicializado")
            except Exception as e:
                error_msg = f"Data Processor: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
                self.data_processor = None
        else:
            self.data_processor = None
        
        # Semantic Router (para classifica√ß√£o inteligente de inten√ß√µes via embeddings)
        if SEMANTIC_ROUTER_AVAILABLE:
            try:
                self.semantic_router = SemanticRouter()
                self.logger.info("‚úÖ Semantic Router inicializado (classifica√ß√£o via embeddings)")
                # Removido: use_semantic_routing obsoleto
            except Exception as e:
                error_msg = f"Semantic Router: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
                self.semantic_router = None
                # Removido: use_semantic_routing obsoleto
        else:
            self.semantic_router = None
            # Removido: use_semantic_routing obsoleto
            self.logger.warning("‚ö†Ô∏è Semantic Router n√£o dispon√≠vel, usando roteamento est√°tico")
        
        # Log do resultado da inicializa√ß√£o
        if self.agents or self.data_processor:
            self.logger.info(f"üöÄ Orquestrador inicializado com {len(self.agents)} agentes")
            if initialization_errors:
                self.logger.warning(f"‚ö†Ô∏è {len(initialization_errors)} componentes falharam na inicializa√ß√£o")
        else:
            self.logger.error("‚ùå Nenhum agente foi inicializado com sucesso")
            if initialization_errors:
                raise AgentError(
                    self.name, 
                    f"Falha na inicializa√ß√£o de todos os componentes: {'; '.join(initialization_errors)}"
                )
    
    def _detect_visualization_type(self, query: str) -> Optional[str]:
        """Detecta se a query solicita algum tipo de visualiza√ß√£o.

        Retorna o tipo identificado (ex: 'histogram', 'bar') ou None.
        M√©todo simples baseado em palavras-chave; mant√©m baixo custo e alta
        previsibilidade.
        """
        if not query:
            return None
        q = query.lower()
        for vtype, keywords in self._viz_keywords.items():
            for kw in keywords:
                if kw in q:
                    return vtype
        return None
    
    def _check_embeddings_data_availability(self) -> bool:
        """Verifica se existem dados na tabela embeddings (CONFORMIDADE)."""
        if not SUPABASE_CLIENT_AVAILABLE or not supabase:
            return False
        
        try:
            result = supabase.table('embeddings').select('id').limit(1).execute()
            has_data = bool(result.data)
            
            if has_data:
                self.logger.info("‚úÖ Dados encontrados na tabela embeddings")
            else:
                self.logger.warning("‚ö†Ô∏è Nenhum dado encontrado na tabela embeddings")
            
            return has_data
        except Exception as e:
            self.logger.error(f"Erro ao verificar dados embeddings: {str(e)}")
            return False
    
    def _ensure_embeddings_compliance(self) -> bool:
        """Garante conformidade com regra embeddings-only.
        
        Returns:
            True se dados est√£o dispon√≠veis via embeddings
        """
        if self._check_embeddings_data_availability():
            return True
        
        self.logger.error("‚ö†Ô∏è VIOLA√á√ÉO DE CONFORMIDADE: Dados n√£o dispon√≠veis via embeddings!")
        self.logger.error("‚ö†Ô∏è Sistema deve funcionar APENAS com dados da tabela embeddings!")
        return False

    def _requires_embeddings(self, query_type: 'QueryType', query: Optional[str] = None) -> bool:
        """Determina se a consulta exige dados indexados em embeddings.

        Regras:
        - Sempre requer para: CSV_ANALYSIS, RAG_SEARCH, HYBRID
        - Para LLM_ANALYSIS: heur√≠stica por palavras-chave de dados
        - Para GENERAL/UNKNOWN/DATA_LOADING: n√£o requer
        """
        if query_type in [QueryType.CSV_ANALYSIS, QueryType.RAG_SEARCH, QueryType.HYBRID]:
            return True

        if query_type == QueryType.LLM_ANALYSIS and query:
            q = query.lower()
            data_keywords = [
                'coluna', 'colunas', 'vari√°veis', 'variaveis', 'estat√≠stica', 'estatistica',
                'distribui√ß√£o', 'distribuicao', 'correla√ß√£o', 'correlacao', 'missing', 'nulos',
                'csv', 'arquivo', 'dataset', 'base de dados', 'planilha', 'dados do arquivo'
            ]
            return any(kw in q for kw in data_keywords)

        return False
    
    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Processa consulta determinando agente(s) apropriado(s).
        
        ‚ö†Ô∏è CONFORMIDADE: Prioriza dados da tabela embeddings.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional (file_path, dados, configura√ß√µes)
        
        Returns:
            Resposta consolidada do sistema
        """
        self.logger.info(f"üéØ Processando consulta: '{query[:50]}...'")
        
        try:
            # 1. Adicionar √† hist√≥ria da conversa (compatibilidade)
            self.conversation_history.append({
                "type": "user_query",
                "query": query,
                "timestamp": self._get_timestamp(),
                "context": context
            })
            
            # 2. Analisar tipo da consulta
            query_type = self._classify_query(query, context)
            self.logger.info(f"üìù Tipo de consulta identificado: {query_type.value}")

            # 2.1 Verificar conformidade apenas quando necess√°rio
            if self._requires_embeddings(query_type, query):
                if not self._ensure_embeddings_compliance():
                    return {
                        'success': False,
                        'error': 'Dados n√£o dispon√≠veis via embeddings. Sistema em conformidade apenas com dados indexados.',
                        'message': 'Por favor, certifique-se de que os dados foram adequadamente indexados na tabela embeddings.',
                        'suggestion': 'Execute o processo de ingest√£o para indexar os dados primeiro.'
                    }
            
            # 3. Processar baseado no tipo
            if query_type == QueryType.CSV_ANALYSIS:
                result = self._handle_csv_analysis(query, context)
            elif query_type == QueryType.RAG_SEARCH:
                result = self._handle_rag_search(query, context)
            elif query_type == QueryType.NFE_ANALYSIS:
                result = self._handle_nfe_analysis(query, context)
            elif query_type == QueryType.DATA_LOADING:
                result = self._handle_data_loading(query, context)
            elif query_type == QueryType.LLM_ANALYSIS:
                result = self._handle_llm_analysis(query, context)
            elif query_type == QueryType.HYBRID:
                result = self._handle_hybrid_query(query, context)
            elif query_type == QueryType.GENERAL:
                result = self._handle_general_query(query, context)
            else:
                result = self._handle_unknown_query(query, context)
            
            # 4. Adicionar √† hist√≥ria
            self.conversation_history.append({
                "type": "system_response",
                "response": result,
                "timestamp": self._get_timestamp()
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro no processamento: {str(e)}")
            return self._build_response(
                f"‚ùå Erro no processamento da consulta: {str(e)}",
                metadata={
                    "error": True,
                    "query_type": "error",
                    "agents_used": []
                }
            )
    
    def _check_data_availability(self) -> bool:
        """Verifica se h√° dados dispon√≠veis na base de dados.
        
        Returns:
            True se h√° dados carregados, False caso contr√°rio
        """
        # 1. Verificar contexto em mem√≥ria primeiro (mais r√°pido)
        if self.current_data_context.get("csv_loaded", False):
            self.logger.debug("‚úÖ Dados encontrados no contexto em mem√≥ria")
            return True
        
        # 2. Verificar dados na base de dados Supabase
        if SUPABASE_CLIENT_AVAILABLE and supabase:
            try:
                # Verificar se h√° dados na tabela embeddings
                result = supabase.table('embeddings').select('id').limit(1).execute()
                if result.data and len(result.data) > 0:
                    self.logger.debug("‚úÖ Dados encontrados na tabela embeddings")
                    # Atualizar contexto em mem√≥ria para pr√≥ximas consultas
                    self.current_data_context["csv_loaded"] = True
                    self.current_data_context["data_source"] = "database_embeddings"
                    return True
                
                # Verificar se h√° dados na tabela chunks
                result = supabase.table('chunks').select('id').limit(1).execute()
                if result.data and len(result.data) > 0:
                    self.logger.debug("‚úÖ Dados encontrados na tabela chunks")
                    # Atualizar contexto em mem√≥ria para pr√≥ximas consultas
                    self.current_data_context["csv_loaded"] = True
                    self.current_data_context["data_source"] = "database_chunks"
                    return True
                
                self.logger.debug("‚ùå Nenhum dado encontrado nas tabelas da base de dados")
                return False
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao verificar dados na base: {str(e)}")
                return False
        else:
            self.logger.debug("‚ö†Ô∏è Cliente Supabase n√£o dispon√≠vel")
            return False
    
    def _detect_visualization_need(self, query: str) -> Optional[str]:
        """
        Detecta se a query do usu√°rio requer visualiza√ß√£o gr√°fica.
        
        Args:
            query: Pergunta do usu√°rio
            
        Returns:
            Tipo de gr√°fico necess√°rio ou None
        """
        try:
            from src.tools.graph_generator import detect_visualization_need
            viz_type = detect_visualization_need(query)
            if viz_type:
                self.logger.info(f"üé® Visualiza√ß√£o detectada: {viz_type}")
            return viz_type
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao detectar visualiza√ß√£o: {e}")
            return None
    
    def _retrieve_data_context_from_supabase(self) -> Optional[Dict[str, Any]]:
        """Recupera contexto de dados armazenados no Supabase.
        
        Returns:
            Dicion√°rio com informa√ß√µes sobre os dados ou None se n√£o conseguir recuperar
        """
        if not SUPABASE_CLIENT_AVAILABLE or not supabase:
            return None
            
        try:
            # CORRE√á√ÉO: Recuperar dados da tabela embeddings (n√£o chunks)
            embeddings_result = supabase.table('embeddings').select('chunk_text, metadata').limit(10).execute()
            
            if not embeddings_result.data:
                self.logger.debug("‚ùå Nenhum embedding encontrado para an√°lise")
                return None
            
            # Analisar chunk_text para extrair informa√ß√µes sobre a estrutura dos dados
            total_embeddings = len(embeddings_result.data)
            sample_chunks = []
            columns_found = set()
            dataset_info = {}
            
            for embedding in embeddings_result.data:
                chunk_text = embedding.get('chunk_text', '')
                metadata = embedding.get('metadata', {})
                
                # Coletar amostra dos chunks para an√°lise
                if chunk_text:
                    sample_chunks.append(chunk_text[:200])  # Primeiros 200 caracteres
                
                # Extrair informa√ß√µes gen√©ricas dos chunks sobre dataset
                # Detectar nome do arquivo CSV
                import re
                csv_match = re.search(r'([\w-]+\.csv)', chunk_text)
                if csv_match:
                    dataset_info['dataset_name'] = csv_match.group(1)
                
                # Sistema gen√©rico - sem detec√ß√£o espec√≠fica de tipo
                dataset_info['type'] = 'general'
                
                # Tentar extrair informa√ß√µes de colunas dos chunks
                if 'colunas:' in chunk_text.lower() or 'columns:' in chunk_text.lower():
                    # Procurar por padr√µes de colunas no texto
                    import re
                    col_patterns = re.findall(r'\b[A-Za-z_][A-Za-z0-9_]*\b', chunk_text)
                    for pattern in col_patterns:
                        if len(pattern) > 2 and not pattern.lower() in ['dataset', 'chunk', 'transacoes', 'linhas']:
                            columns_found.add(pattern)
            
            # Construir contexto baseado nos dados encontrados
            context = {
                'csv_loaded': True,
                'data_source': 'database_embeddings',
                'csv_analysis': f"Dados encontrados na base vetorial: {total_embeddings} embeddings dispon√≠veis."
            }
            
            if dataset_info.get('dataset_name'):
                context['file_path'] = dataset_info['dataset_name']
                context['csv_analysis'] += f" Dataset: {dataset_info['dataset_name']}"
                
                # üîß SISTEMA GEN√âRICO: Calcular estat√≠sticas reais para QUALQUER CSV
                if PYTHON_ANALYZER_AVAILABLE and python_analyzer:
                    try:
                        self.logger.info("üî¢ Calculando estat√≠sticas reais com Python Analyzer...")
                        real_stats = python_analyzer.calculate_real_statistics("all")
                        
                        if "error" not in real_stats:
                            # Usar estat√≠sticas reais ao inv√©s de estimativas
                            context['csv_analysis'] += f"\n\nüìä ESTAT√çSTICAS REAIS (do chunk_text parseado):"
                            context['csv_analysis'] += f"\n- Total de registros: {real_stats['total_records']:,}"
                            context['csv_analysis'] += f"\n- Total de colunas: {real_stats['total_columns']}"
                            
                            if 'tipos_dados' in real_stats:
                                tipos = real_stats['tipos_dados']
                                # ‚úÖ INFORMA√á√ÉO ESTRUTURADA GEN√âRICA DAS COLUNAS (funciona com qualquer CSV)
                                context['csv_analysis'] += f"\n\nüìã COLUNAS RECONSTRU√çDAS DA TABELA EMBEDDINGS (chunk_text parseado):"
                                context['csv_analysis'] += f"\n- Colunas totais: {real_stats['total_columns']}"
                                context['csv_analysis'] += f"\n- Lista completa de colunas: {real_stats['columns']}"
                                context['csv_analysis'] += f"\n\nüìä TIPOS DE DADOS (baseado em dtypes reais do DataFrame parseado):"
                                context['csv_analysis'] += f"\n- Num√©ricas ({tipos['total_numericos']}): {tipos['numericos']}"
                                context['csv_analysis'] += f"\n- Categ√≥ricas ({tipos['total_categoricos']}): {tipos['categoricos']}"
                                if tipos.get('datetime'):
                                    context['csv_analysis'] += f"\n- Temporais ({tipos['total_datetime']}): {tipos['datetime']}"
                                
                                context['columns_summary'] = f"Num√©ricos: {', '.join(tipos['numericos'][:5])}{'...' if len(tipos['numericos']) > 5 else ''} ({tipos['total_numericos']} colunas), Categ√≥ricos: {', '.join(tipos['categoricos'])}"
                            
                            # Sistema gen√©rico - estat√≠sticas j√° inclu√≠das em real_stats
                            # Sem l√≥gica espec√≠fica por tipo de dataset
                            
                            self.logger.info("‚úÖ Estat√≠sticas reais calculadas com sucesso")
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Erro no Python Analyzer: {real_stats.get('error')}")
                            # N√£o h√° fallback com colunas hardcoded - sistema deve funcionar genericamente
                    
                    except Exception as e:
                        self.logger.error(f"‚ùå Erro ao calcular estat√≠sticas reais: {str(e)}")
                        # Sem fallback hardcoded - sistema gen√©rico
                        context['csv_analysis'] += "\n\n‚ö†Ô∏è N√£o foi poss√≠vel calcular estat√≠sticas detalhadas"
                else:
                    # Python Analyzer n√£o dispon√≠vel
                    self.logger.warning("‚ö†Ô∏è Python Analyzer n√£o dispon√≠vel")
                    context['csv_analysis'] += "\n\n‚ö†Ô∏è Python Analyzer n√£o configurado"
            
            if columns_found:
                context['csv_analysis'] += f" Colunas identificadas: {', '.join(list(columns_found)[:10])}"
            
            # Tentar recuperar uma amostra dos dados reais usando RAG
            if "rag" in self.agents:
                try:
                    sample_query = "tipos dados colunas num√©ricos categ√≥ricos"  # Query mais espec√≠fica e curta
                    rag_result = self.agents["rag"].process(sample_query, {})
                    if rag_result and not rag_result.get("metadata", {}).get("error", False):
                        # Adicionar informa√ß√µes do RAG ao contexto (LIMITADO)
                        rag_content = rag_result.get("content", "")
                        if rag_content and len(rag_content) > 50:  # Se temos conte√∫do significativo
                            # LIMITA√á√ÉO: Usar apenas os primeiros 300 caracteres para evitar token overflow
                            context['csv_analysis'] += f"\n\nInforma√ß√µes dos dados:\n{rag_content[:300]}..."
                            self.logger.info("‚úÖ Contexto enriquecido com dados do RAG (resumido)")
                except Exception as e:
                    self.logger.debug(f"‚ö†Ô∏è Erro ao recuperar amostra via RAG: {str(e)}")
                    # Sistema gen√©rico - sem informa√ß√µes hardcoded
                    context['csv_analysis'] += "\n\n‚úÖ Dados carregados do banco vetorial"
            
            return context
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao recuperar contexto do Supabase: {str(e)}")
            return None
    
    def _classify_query(self, query: str, context: Optional[Dict[str, Any]]) -> QueryType:
        """Classifica o tipo de consulta usando LLM para decis√£o inteligente.
        
        FLUXO INTELIGENTE (SEM HARDCODING):
        1. Usa LLM para classificar a inten√ß√£o da pergunta
        2. LLM decide se precisa de dados do dataset ou √© pergunta geral
        3. Apenas fallback usa keywords (se LLM falhar)
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional
        
        Returns:
            Tipo da consulta identificado
        """
        # ========================================
        # ETAPA 1: CLASSIFICA√á√ÉO INTELIGENTE VIA LLM
        # ========================================
        if self.llm_manager:
            try:
                self.logger.info("üß† Usando LLM para classifica√ß√£o inteligente da consulta...")
                
                # Prompt para o LLM classificar a inten√ß√£o
                classification_prompt = f"""Voc√™ DEVE responder com APENAS UMA palavra. Nada mais.

PERGUNTA: "{query}"

INSTRU√á√ïES OBRIGAT√ìRIAS:
1. Leia a pergunta acima
2. Escolha EXATAMENTE uma destas palavras (copie exatamente):
   - GENERAL (sauda√ß√µes, apresenta√ß√µes, conversa geral)
   - NFE_ANALYSIS (perguntas sobre Notas Fiscais, CFOP, NCM, impostos)
   - CSV_ANALYSIS (an√°lises de dados: soma, m√©dia, m√°ximo, m√≠nimo, distribui√ß√£o, gr√°ficos)
   - RAG_SEARCH (buscar informa√ß√µes espec√≠ficas no banco vetorial)
   - DATA_LOADING (carregar/importar arquivos)
3. Responda SOMENTE com a palavra escolhida
4. N√ÉO adicione explica√ß√µes, frases ou pontua√ß√£o

EXEMPLOS:
"Oi" -> GENERAL
"Sobre o que √© o dataset?" -> CSV_ANALYSIS
"Qual CFOP de devolu√ß√£o?" -> NFE_ANALYSIS
"Busque no contexto" -> RAG_SEARCH
"Carregue vendas.csv" -> DATA_LOADING

SUA RESPOSTA (uma palavra apenas):"""

                config = LLMConfig(temperature=0.1, max_tokens=20)
                classification_result = self.llm_manager.chat(classification_prompt, config)
                
                if classification_result.success:
                    classification = classification_result.content.strip().upper()
                    self.logger.info(f"üéØ LLM classificou como: {classification}")
                    
                    # Mapear resposta do LLM para QueryType
                    if 'GENERAL' in classification:
                        return QueryType.GENERAL
                    elif 'NFE_ANALYSIS' in classification or 'NFE' in classification:
                        return QueryType.NFE_ANALYSIS
                    elif 'CSV_ANALYSIS' in classification or 'CSV' in classification:
                        return QueryType.CSV_ANALYSIS
                    elif 'RAG_SEARCH' in classification or 'RAG' in classification:
                        return QueryType.RAG_SEARCH
                    elif 'DATA_LOADING' in classification or 'LOADING' in classification:
                        return QueryType.DATA_LOADING
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Classifica√ß√£o LLM amb√≠gua: {classification}")
                        # Continua para fallback
                else:
                    self.logger.warning(f"‚ö†Ô∏è LLM falhou na classifica√ß√£o: {classification_result.error}")
                    
            except Exception as e:
                self.logger.error(f"‚ùå Erro na classifica√ß√£o LLM: {str(e)}")
                self.logger.info("üîÑ Fallback para roteamento sem√¢ntico/est√°tico")
        
        # ========================================
        # ETAPA 2: FALLBACK - ROTEAMENTO SEM√ÇNTICO
        # ========================================
        if self.semantic_router:
            try:
                self.logger.info("üß† Usando roteamento sem√¢ntico via embeddings...")
                
                routing_result = self.semantic_router.route(query)
                self.logger.info(f"üìç Roteamento sem√¢ntico: {routing_result}")
                
                route = routing_result.get('route', 'unknown')
                confidence = routing_result.get('confidence', 0.0)
                
                if confidence >= 0.7:
                    self.logger.info(f"‚úÖ Classifica√ß√£o sem√¢ntica com alta confian√ßa ({confidence:.2f})")
                    
                    route_mapping = {
                        'statistical_analysis': QueryType.CSV_ANALYSIS,
                        'data_visualization': QueryType.CSV_ANALYSIS,
                        'contextual_embedding': QueryType.RAG_SEARCH,
                        'data_loading': QueryType.DATA_LOADING,
                        'llm_generic': QueryType.LLM_ANALYSIS,
                        'unknown': None
                    }
                    
                    query_type = route_mapping.get(route)
                    
                    if query_type:
                        self.logger.info(f"üéØ Rota sem√¢ntica mapeada: {route} ‚Üí {query_type.value}")
                        return query_type
                else:
                    self.logger.warning(f"‚ö†Ô∏è Confian√ßa baixa ({confidence:.2f}), usando fallback est√°tico")
                    
            except Exception as e:
                self.logger.error(f"‚ùå Erro no roteamento sem√¢ntico: {str(e)}")
                self.logger.info("üîÑ Fallback para roteamento est√°tico")
        
        # ========================================
        # ETAPA 2: FALLBACK - ROTEAMENTO EST√ÅTICO
        # ========================================
        self.logger.info("üìã Usando roteamento est√°tico por palavras-chave...")
        
        query_lower = query.lower()
        
        # Verificar se √© solicita√ß√£o de visualiza√ß√£o
        viz_type = self._detect_visualization_need(query)
        if viz_type:
            self.logger.info(f"üìä Visualiza√ß√£o detectada: {viz_type}")
            # Adicionar flag ao contexto para processamento posterior
            if context is None:
                context = {}
            context['visualization_requested'] = viz_type
        
        # Palavras-chave para cada tipo de consulta
        csv_keywords = [
            'csv', 'tabela', 'dados', 'an√°lise', 'estat√≠stica', 'correla√ß√£o',
            'gr√°fico', 'plot', 'visualiza√ß√£o', 'resumo', 'describe', 'dataset',
            'colunas', 'linhas', 'm√©dia', 'mediana', 'fraude', 'outlier',
            'tipos de dados', 'num√©ricos', 'categ√≥ricos', 'distribui√ß√£o',
            'intervalo', 'm√≠nimo', 'm√°ximo', 'min', 'max', 'range', 'amplitude',
            'vari√¢ncia', 'desvio', 'percentil', 'quartil', 'valores',
            'vari√°vel', 'vari√°veis', 'features', 'atributos', 'estat√≠sticas',
            'padr√£o', 'padr√µes', 'tend√™ncia', 'tend√™ncias', 'temporal', 'temporais',
            'tempo', 's√©rie', 's√©ries', 'comportamento', 'anomalia', 'an√¥malo',
            'frequente', 'frequentes', 'frequ√™ncia', 'comum', 'raro', 'raros',
            'moda', 'contagem', 'count', 'value_counts', 'top', 'bottom',
            'cluster', 'clusters', 'agrupamento', 'agrupamentos', 'grupos',
            'kmeans', 'k-means', 'dbscan', 'hier√°rquico', 'hierarquico',
            'segmenta√ß√£o', 'segmentacao'
        ]
        
        rag_keywords = [
            'buscar', 'procurar', 'encontrar', 'pesquisar', 'consultar',
            'conhecimento', 'base', 'documento', 'texto', 'similar',
            'contexto', 'embedding', 'sem√¢ntica', 'retrieval'
        ]
        
        data_keywords = [
            'carregar', 'upload', 'importar', 'abrir', 'arquivo',
            'dados sint√©ticos', 'gerar dados', 'criar dados', 'load'
        ]
        
        llm_keywords = [
            'explicar', 'explique', 'interpretar', 'interprete', 'insight', 'insights', 
            'conclus√£o', 'conclus√µes', 'recomenda√ß√£o', 'recomenda√ß√µes', 'recomende',
            'sugest√£o', 'sugest√µes', 'sugira', 'opini√£o', 'an√°lise detalhada', 
            'relat√≥rio', 'sum√°rio', 'resume', 'resumo detalhado', 
            'previs√£o', 'hip√≥tese', 'teoria', 'tire', 'conclua',
            'avalie', 'considere', 'entenda', 'compreenda', 'descoberta',
            'descobrimentos', 'suspeito',
            'detalhado', 'profundo', 'aprofunde', 'discuta', 'comente', 'o que',
            'quais', 'como', 'por que', 'porque'
        ]
        
        general_keywords = [
            'ol√°', 'oi', 'ajuda', 'como', 'o que', 'qual', 'quando',
            'onde', 'por que', 'definir', 'status', 'sistema',
            'bom dia', 'boa tarde', 'boa noite', 'ola', 'ei',
            'meu nome', 'me chamo', 'sou o', 'sou a', 'prazer', 'obrigado', 'obrigada'
        ]
        
        # Verificar contexto de arquivo
        has_file_context = context and 'file_path' in context
        
        # CORRE√á√ÉO: Verificar se h√° dados carregados no Supabase
        has_supabase_data = self._check_data_availability()
        
        # Classificar baseado em palavras-chave e contexto
        csv_score = sum(1 for kw in csv_keywords if kw in query_lower)
        rag_score = sum(1 for kw in rag_keywords if kw in query_lower)
        data_score = sum(1 for kw in data_keywords if kw in query_lower)
        llm_score = sum(3 for kw in llm_keywords if kw in query_lower)  # Peso triplicado para LLM
        general_score = sum(2 for kw in general_keywords if kw in query_lower)  # Peso 2 para sauda√ß√µes/apresenta√ß√µes
        
        # PRIORIDADE: Se h√° visualiza√ß√£o detectada, sempre usar CSV_ANALYSIS
        # porque apenas o EmbeddingsAnalysisAgent tem o m√©todo _handle_visualization_query
        if viz_type and has_supabase_data:
            self.logger.info("üé® Redirecionando para CSV analysis (visualiza√ß√£o solicitada)")
            return QueryType.CSV_ANALYSIS

        # CORRE√á√ÉO ABSOLUTA: Se a query cont√©m termos de intervalo, m√≠nimo, m√°ximo, range, amplitude, SEMPRE usar CSV_ANALYSIS
        interval_terms = ['intervalo', 'm√≠nimo', 'm√°ximo', 'range', 'amplitude']
        if any(term in query_lower for term in interval_terms):
            self.logger.info("üîí For√ßando roteamento para CSV analysis por conter termos de intervalo/m√≠nimo/m√°ximo/range/amplitude")
            return QueryType.CSV_ANALYSIS

        # PRIORIDADE 2: Se h√° dados no Supabase E score CSV alto, usar CSV_ANALYSIS (RAGDataAgent)
        # Isso permite perguntas sobre estat√≠sticas, intervalos, distribui√ß√£o irem para o RAGDataAgent
        if has_supabase_data and csv_score >= 2:
            self.logger.info("üìä Redirecionando para CSV analysis (dados no Supabase + an√°lise estat√≠stica detectada)")
            return QueryType.CSV_ANALYSIS
        
        # Adicionar peso do contexto
        if has_file_context:
            if any(ext in str(context.get('file_path', '')).lower() for ext in ['.csv', '.xlsx', '.json']):
                csv_score += 1
        
        # Verificar se precisa de m√∫ltiplos agentes
        scores = [csv_score, rag_score, data_score, llm_score]
        high_scores = [s for s in scores if s >= 2]
        
        # Se LLM tem score alto, priorizar sobre hybrid
        if llm_score >= 3:
            return QueryType.LLM_ANALYSIS
        
        if len(high_scores) >= 2:
            return QueryType.HYBRID
        
        # Determinar tipo baseado na maior pontua√ß√£o
        max_score = max(csv_score, rag_score, data_score, llm_score, general_score)
        
        if max_score == 0:
            return QueryType.UNKNOWN
        elif max_score == csv_score:
            return QueryType.CSV_ANALYSIS
        elif max_score == rag_score:
            return QueryType.RAG_SEARCH
        elif max_score == data_score:
            return QueryType.DATA_LOADING
        elif max_score == llm_score:
            return QueryType.LLM_ANALYSIS
        else:
            return QueryType.GENERAL
    
    def _handle_csv_analysis(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Delega an√°lise CSV para o agente especializado.
        
        LOGGING: Registra decis√£o de roteamento e agente utilizado.
        """
        if "csv" not in self.agents:
            return self._build_response(
                "‚ùå Agente de an√°lise CSV n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        # Log da decis√£o de delega√ß√£o
        self.logger.info("üìä Delegando para agente CSV (EmbeddingsAnalysisAgent)")
        self.logger.info(f"üîç Query: '{query[:80]}...'")
        
        # Preparar contexto para o agente CSV
        csv_context = context or {}
        
        # Se h√° dados carregados no orquestrador, passar para o agente
        if self.current_data_context:
            csv_context.update(self.current_data_context)
            self.logger.debug(f"üì¶ Contexto de dados atual: {list(self.current_data_context.keys())}")
        
        # Executar processamento no agente especializado (s√≠ncrono)
        try:
            result = self.agents["csv"].process(query, csv_context)
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente CSV: {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente CSV: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )
        
        # Log do resultado
        if result.get("metadata", {}).get("error"):
            self.logger.error(f"‚ùå Erro no agente CSV: {result.get('response', 'Erro desconhecido')}")
        else:
            self.logger.info("‚úÖ An√°lise CSV conclu√≠da com sucesso")
        
        # Atualizar contexto se dados foram carregados
        if result.get("metadata") and not result["metadata"].get("error"):
            self.current_data_context.update(result["metadata"])
        
        return self._enhance_response(result, ["embeddings_analyzer"])
    
    def _handle_rag_search(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Delega busca sem√¢ntica para o agente RAG."""
        if "rag" not in self.agents:
            return self._build_response(
                "‚ùå Agente RAG n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        self.logger.info("üîç Delegando para agente RAG")
        
        try:
            # Garantir que o contexto inclua ingestion_id/source_id do dataset ativo
            context = context or {}
            # Buscar do contexto atual do orquestrador se dispon√≠vel
            ingestion_id = self.current_data_context.get('ingestion_id')
            source_id = self.current_data_context.get('source_id')
            if ingestion_id:
                context['ingestion_id'] = ingestion_id
            if source_id:
                context['source_id'] = source_id
            result = self.agents["rag"].process(query, context)
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente RAG: {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente RAG: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )

        return self._enhance_response(result, ["rag"])

    def _handle_nfe_analysis(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Delega an√°lise fiscal de NFe para o agente especializado.
        
        Este m√©todo roteia perguntas sobre Notas Fiscais Eletr√¥nicas para o
        NFeTaxSpecialistAgent, que possui conhecimento especializado em:
        - CFOP e NCM
        - Legisla√ß√£o tribut√°ria
        - C√°lculo de tributos (ICMS, IPI, PIS, COFINS)
        - Detec√ß√£o de anomalias fiscais
        """
        if "nfe" not in self.agents:
            return self._build_response(
                "‚ùå Agente NFe Tax Specialist n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        self.logger.info("üìã Delegando para agente NFe Tax Specialist")
        self.logger.info(f"üîç Query: '{query[:80]}...'")
        
        try:
            # NFeTaxSpecialistAgent espera query direta
            result = self.agents["nfe"].process(query, context or {})
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente NFe: {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente NFe: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )
        
        if result.get("metadata", {}).get("error"):
            self.logger.error(f"‚ùå Erro no agente NFe: {result.get('content', 'Erro desconhecido')}")
        else:
            self.logger.info("‚úÖ An√°lise NFe conclu√≠da com sucesso")
        
        return self._enhance_response(result, ["nfe_tax_specialist"])

    # ========================================================================
    # VERS√ÉO ASS√çNCRONA DOS HANDLERS (USADA POR process_with_persistent_memory)
    # ========================================================================
    async def _handle_csv_analysis_async(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Vers√£o async que aguarda agentes ass√≠ncronos quando necess√°rio."""
        if "csv" not in self.agents:
            return self._build_response(
                "‚ùå Agente de an√°lise CSV n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )

        self.logger.info("üìä Delegando para agente CSV (RAGDataAgent) [async]")
        # Para queries de intervalo, limpar contexto para evitar polui√ß√£o por hist√≥rico/mem√≥ria
        interval_terms = ['intervalo', 'm√≠nimo', 'm√°ximo', 'range', 'amplitude']
        query_lower = query.lower()
        if any(term in query_lower for term in interval_terms):
            csv_context = {}  # contexto limpo, sem mem√≥ria/hist√≥rico
            self.logger.info("üßπ Contexto limpo aplicado para consulta de intervalo/min/max/range/amplitude")
        else:
            csv_context = context or {}
            if self.current_data_context:
                csv_context.update(self.current_data_context)
        
        # ‚úÖ CORRE√á√ÉO: Detectar solicita√ß√£o de visualiza√ß√£o e setar flag no contexto
        viz_type = self._detect_visualization_type(query)
        if viz_type:
            csv_context['visualization_requested'] = True
            csv_context['visualization_type'] = viz_type
            self.logger.info(f"üìä Flag de visualiza√ß√£o setada: {viz_type}")

        try:
            # RAGDataAgent.process() √© async e requer session_id opcional
            session_id = csv_context.get('session_id') or self._current_session_id
            result = await self.agents["csv"].process(query, csv_context, session_id=session_id)
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente CSV (async): {e}", exc_info=True)
            return self._build_response(
                f"‚ùå Erro ao executar agente CSV: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )

        if result.get("metadata", {}).get("error"):
            self.logger.error(f"‚ùå Erro no agente CSV: {result.get('response', result.get('content', 'Erro desconhecido'))}")
        else:
            self.logger.info("‚úÖ An√°lise CSV conclu√≠da com sucesso [async]")

        if result.get("metadata") and not result["metadata"].get("error"):
            self.current_data_context.update(result["metadata"])

        return self._enhance_response(result, ["rag_data_agent"])

    async def _handle_rag_search_async(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        if "rag" not in self.agents:
            return self._build_response(
                "‚ùå Agente RAG n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )

        self.logger.info("üîç Delegando para agente RAG [async]")
        try:
            result_candidate = self.agents["rag"].process(query, context)
            import inspect
            if inspect.isawaitable(result_candidate):
                result = await result_candidate
            else:
                result = result_candidate
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente RAG (async): {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente RAG: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )

        return self._enhance_response(result, ["rag"])

    async def _handle_nfe_analysis_async(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Vers√£o async do handler NFe."""
        if "nfe" not in self.agents:
            return self._build_response(
                "‚ùå Agente NFe Tax Specialist n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )

        self.logger.info("üìã Delegando para agente NFe Tax Specialist [async]")
        try:
            result_candidate = self.agents["nfe"].process(query, context or {})
            import inspect
            if inspect.isawaitable(result_candidate):
                result = await result_candidate
            else:
                result = result_candidate
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente NFe (async): {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente NFe: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )

        return self._enhance_response(result, ["nfe_tax_specialist"])

    async def _process_async(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Vers√£o ass√≠ncrona de process() utilizada por process_with_persistent_memory."""
        self.logger.info(f"üéØ [async] Processando consulta: '{query[:50]}...'")

        try:
            # Adicionar √† hist√≥ria compatibilidade
            self.conversation_history.append({
                "type": "user_query",
                "query": query,
                "timestamp": self._get_timestamp(),
                "context": context
            })

            query_type = self._classify_query(query, context)
            self.logger.info(f"üìù [async] Tipo de consulta identificado: {query_type.value}")

            # Verificar conformidade apenas quando necess√°rio
            if self._requires_embeddings(query_type, query):
                if not self._ensure_embeddings_compliance():
                    return {
                        'success': False,
                        'error': 'Dados n√£o dispon√≠veis via embeddings. Sistema em conformidade apenas com dados indexados.',
                        'message': 'Por favor, certifique-se de que os dados foram adequadamente indexados na tabela embeddings.',
                        'suggestion': 'Execute o processo de ingest√£o para indexar os dados primeiro.'
                    }

            # Processar baseado no tipo (usar vers√µes async quando dispon√≠vel)
            if query_type == QueryType.CSV_ANALYSIS:
                result = await self._handle_csv_analysis_async(query, context)
            elif query_type == QueryType.RAG_SEARCH:
                result = await self._handle_rag_search_async(query, context)
            elif query_type == QueryType.NFE_ANALYSIS:
                result = await self._handle_nfe_analysis_async(query, context)
            elif query_type == QueryType.DATA_LOADING:
                result = self._handle_data_loading(query, context)
            elif query_type == QueryType.LLM_ANALYSIS:
                # LLM analysis pode chamar agentes sync/async internamente
                # Reusar implementa√ß√£o s√≠ncrona e permitir que ela chame agentes sync
                result = self._handle_llm_analysis(query, context)
            elif query_type == QueryType.HYBRID:
                result = self._handle_hybrid_query(query, context)
            elif query_type == QueryType.GENERAL:
                result = self._handle_general_query(query, context)
            else:
                result = self._handle_unknown_query(query, context)

            # Adicionar resposta ao hist√≥rico
            self.conversation_history.append({
                "type": "system_response",
                "response": result,
                "timestamp": self._get_timestamp()
            })

            return result

        except Exception as e:
            self.logger.error(f"Erro no processamento async: {str(e)}")
            return self._build_response(
                f"‚ùå Erro no processamento da consulta: {str(e)}",
                metadata={"error": True, "query_type": "error", "agents_used": []}
            )
    
    def _handle_data_loading(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa carregamento de dados."""
        if not self.data_processor:
            return self._build_response(
                "‚ùå Sistema de carregamento de dados n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        self.logger.info("üìÅ Processando carregamento de dados")
        
        try:
            # ‚ö†Ô∏è CONFORMIDADE: OrchestratorAgent N√ÉO deve carregar CSV para consultas
            # Este m√©todo deve ser usado apenas para ingest√£o inicial
            self.logger.warning("üö® ATEN√á√ÉO: OrchestratorAgent realizando carregamento de dados!")
            self.logger.warning("üö® Consultas devem usar APENAS a tabela embeddings!")
            
            # Verificar se foi fornecido um arquivo
            if context and 'file_path' in context:
                file_path = context['file_path']
                
                # Carregar dados usando DataProcessor (que deve validar autoriza√ß√£o)
                result = self.data_processor.load_from_file(file_path)
                
                if not result.get('error'):
                    # Armazenar contexto dos dados carregados
                    self.current_data_context = {
                        'file_path': file_path,
                        'data_info': result.get('data_info', {}),
                        'quality_report': result.get('quality_report', {})
                    }
                    
                    # Criar resposta informativa
                    data_info = result.get('data_info', {})
                    quality_report = result.get('quality_report', {})
                    
                    response = f"""‚úÖ **Dados Carregados com Sucesso**

üìÑ **Arquivo:** {file_path}
üìä **Dimens√µes:** {data_info.get('rows', 0):,} linhas √ó {data_info.get('columns', 0)} colunas
‚≠ê **Qualidade:** {quality_report.get('overall_score', 0):.1f}/100

**Pr√≥ximos passos dispon√≠veis:**
‚Ä¢ An√°lise explorat√≥ria: "fa√ßa um resumo dos dados"
‚Ä¢ Correla√ß√µes: "mostre as correla√ß√µes"  
‚Ä¢ Visualiza√ß√µes: "crie gr√°ficos dos dados"
‚Ä¢ Busca sem√¢ntica: "busque informa√ß√µes sobre fraude"
"""
                    
                    return self._build_response(
                        response,
                        metadata={
                            "agents_used": ["data_processor"],
                            "data_loaded": True,
                            "file_path": file_path,
                            "data_info": data_info,
                            "quality_report": quality_report
                        }
                    )
                else:
                    return self._build_response(
                        f"‚ùå Erro ao carregar dados: {result.get('error', 'Erro desconhecido')}",
                        metadata={"error": True, "agents_used": ["data_processor"]}
                    )
            
            else:
                # Instru√ß√µes de como carregar dados
                response = """üìÅ **Como Carregar Dados**

Para carregar dados, use:
```
context = {"file_path": "caminho/para/seu/arquivo.csv"}
```

**Formatos suportados:**
‚Ä¢ CSV (.csv)
‚Ä¢ Excel (.xlsx) - *em desenvolvimento*
‚Ä¢ JSON (.json) - *em desenvolvimento*

**Dados sint√©ticos dispon√≠veis:**
‚Ä¢ Detec√ß√£o de fraude
‚Ä¢ Dados de vendas  
‚Ä¢ Dados de clientes
‚Ä¢ Dados gen√©ricos
"""
                
                return self._build_response(
                    response,
                    metadata={"agents_used": [], "instructions": True}
                )
        
        except Exception as e:
            self.logger.error(f"Erro no carregamento: {str(e)}")
            return self._build_response(
                f"‚ùå Erro no carregamento de dados: {str(e)}",
                metadata={"error": True, "agents_used": ["data_processor"]}
            )

    def _handle_llm_analysis(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas atrav√©s do LLM Manager com verifica√ß√£o de base de dados."""
        if not self.llm_manager:
            return self._build_response(
                "‚ùå LLM Manager n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        self.logger.info("ü§ñ Delegando para LLM Manager")
        
        # 1. VERIFICA√á√ÉO OBRIGAT√ìRIA: Identificar se consulta requer dados espec√≠ficos
        data_specific_keywords = [
            'tipos de dados', 'colunas', 'vari√°veis', 'estat√≠sticas', 'resumo',
            'distribui√ß√£o', 'correla√ß√£o', 'missing', 'nulos', 'formato',
            'csv', 'arquivo', 'dataset', 'base de dados', 'planilha'
        ]
        
        needs_data_analysis = any(keyword in query.lower() for keyword in data_specific_keywords)
        
        # 2. VERIFICAR ESTADO DOS DADOS
        has_loaded_data = self._check_data_availability()
        has_file_context = bool(context and context.get("file_path"))
        
        self.logger.info(f"üìä An√°lise necess√°ria: {needs_data_analysis}, Dados carregados: {has_loaded_data}, Arquivo no contexto: {has_file_context}")
        
        # 3. L√ìGICA DE DECIS√ÉO BASEADA NO ESTADO
        if needs_data_analysis and not has_loaded_data and not has_file_context:
            # Caso 1: Precisa de dados espec√≠ficos mas n√£o h√° nada carregado
            return self._build_response(
                """‚ùì **Base de Dados Necess√°ria**
                
Sua pergunta requer an√°lise de dados espec√≠ficos, mas n√£o h√° nenhuma base de dados carregada no momento.

**Op√ß√µes dispon√≠veis:**

üî∏ **An√°lise espec√≠fica**: Carregue um arquivo CSV primeiro:
   ‚Ä¢ "carregar arquivo dados.csv"
   ‚Ä¢ "analisar arquivo /caminho/para/arquivo.csv"

üî∏ **Resposta gen√©rica**: Se deseja uma explica√ß√£o geral sobre o conceito, reformule sua pergunta:
   ‚Ä¢ "o que s√£o tipos de dados em geral?"
   ‚Ä¢ "explique conceitos b√°sicos de an√°lise de dados"

**Como posso te ajudar?**""",
                metadata={
                    "error": False, 
                    "agents_used": ["llm_manager"],
                    "requires_data": True,
                    "data_available": False
                }
            )
        
        elif needs_data_analysis and not has_loaded_data and has_file_context:
            # Caso 2: Precisa de dados, tem arquivo no contexto, mas n√£o carregou ainda
            self.logger.info("üîÑ Carregando dados automaticamente para an√°lise espec√≠fica...")
            
            # Tentar carregar dados usando agente CSV
            if "csv" in self.agents:
                try:
                    load_query = f"carregar e analisar estrutura b√°sica"
                    csv_result = self.agents["csv"].process(load_query, context)
                    
                    if csv_result and not csv_result.get("metadata", {}).get("error", False):
                        # Extrair informa√ß√µes do CSV e atualizar contexto
                        self._update_data_context_from_csv_result(csv_result, context)
                        self.logger.info("‚úÖ Dados carregados automaticamente")
                    else:
                        return self._build_response(
                            f"‚ùå N√£o foi poss√≠vel carregar o arquivo: {csv_result.get('content', 'Erro desconhecido')}",
                            metadata={"error": True, "agents_used": ["csv"]}
                        )
                except Exception as e:
                    return self._build_response(
                        f"‚ùå Erro ao carregar arquivo: {str(e)}",
                        metadata={"error": True, "agents_used": ["csv"]}
                    )
            else:
                return self._build_response(
                    "‚ùå Agente CSV n√£o dispon√≠vel para carregar dados",
                    metadata={"error": True, "agents_used": []}
                )
        
        # 4. PREPARAR CONTEXTO PARA LLM
        llm_context = context.copy() if context else {}
        
        # Adicionar dados carregados se dispon√≠veis
        if self.current_data_context:
            llm_context.update(self.current_data_context)
        
        # üîÑ REDIRECIONAMENTO PARA RAG: Se precisa de an√°lise de dados e h√° embeddings no Supabase
        if needs_data_analysis and has_loaded_data:
            self.logger.info("üîÑ Redirecionando para LLM analysis (dados no Supabase detectados)")
            
            # Verificar se deve usar RAG para interpreta√ß√£o sem√¢ntica dos chunks
            if "rag" in self.agents:
                try:
                    # Enriquecer contexto com an√°lise sem√¢ntica via RAG
                    self.logger.info("üìö Usando RAG para interpreta√ß√£o sem√¢ntica dos chunks...")
                    rag_result = self.agents["rag"].process(query, {"include_context": True, "max_results": 5})
                    
                    if rag_result and not rag_result.get("metadata", {}).get("error"):
                        # Adicionar contexto RAG ao LLM context
                        llm_context["rag_context"] = rag_result.get("content", "")
                        llm_context["rag_sources"] = rag_result.get("metadata", {}).get("sources", [])
                        self.logger.info("‚úÖ Contexto enriquecido com dados do RAG (resumido)")
                    else:
                        self.logger.warning("‚ö†Ô∏è RAG n√£o retornou resultados, continuando sem contexto RAG")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao usar RAG: {str(e)}, continuando sem contexto RAG")
            
            # NOVA FUNCIONALIDADE: Recuperar dados do Supabase quando necess√°rio
            if not llm_context.get("csv_analysis"):
                self.logger.info("üîç Recuperando dados da base Supabase para an√°lise...")
                try:
                    # Recuperar informa√ß√µes sobre os dados armazenados
                    supabase_data_context = self._retrieve_data_context_from_supabase()
                    if supabase_data_context:
                        llm_context.update(supabase_data_context)
                        self.logger.info("‚úÖ Contexto de dados recuperado do Supabase")
                    else:
                        self.logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel recuperar contexto de dados do Supabase")
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao recuperar dados do Supabase: {str(e)}")
        
        # 5. CONSTRUIR PROMPT CONTEXTUALIZADO
        prompt = self._build_llm_prompt(query, llm_context, needs_data_analysis)
        
        try:
            # 6. CHAMAR LLM MANAGER com configura√ß√£o otimizada
            config = LLMConfig(temperature=0.2, max_tokens=512)  # Reduzir tokens de resposta
            response = self.llm_manager.chat(prompt, config)
            
            if not response.success:
                raise RuntimeError(response.error)
            
            # 7. APLICAR GUARDRAILS DE VALIDA√á√ÉO
            if GUARDRAILS_AVAILABLE and statistics_guardrails and needs_data_analysis:
                validation_result = statistics_guardrails.validate_response(response.content, llm_context)
                
                if not validation_result.is_valid and validation_result.confidence_score < 0.7:
                    self.logger.warning(f"‚ö†Ô∏è Resposta falhol na valida√ß√£o (score: {validation_result.confidence_score:.2f})")
                    self.logger.warning(f"Issues detectados: {', '.join(validation_result.issues[:3])}")
                    
                    # Se h√° valores corrigidos, tentar nova consulta com corre√ß√µes
                    if validation_result.corrected_values and len(validation_result.issues) <= 3:
                        correction_prompt = statistics_guardrails.generate_correction_prompt(validation_result)
                        
                        # Adicionar corre√ß√µes ao contexto
                        corrected_context = llm_context.copy()
                        corrected_context['correction_prompt'] = correction_prompt
                        
                        # Tentar novamente com corre√ß√µes
                        self.logger.info("üîÑ Tentando nova consulta com corre√ß√µes...")
                        corrected_prompt = self._build_llm_prompt(query, corrected_context, needs_data_analysis)
                        
                        try:
                            config = LLMConfig(temperature=0.1, max_tokens=512)  # Temperatura mais baixa para precis√£o
                            corrected_response = self.llm_manager.chat(corrected_prompt, config)
                            
                            if corrected_response.success:
                                response = corrected_response
                                self.logger.info("‚úÖ Resposta corrigida gerada com sucesso")
                        except Exception as e:
                            self.logger.warning(f"‚ö†Ô∏è Falha na corre√ß√£o autom√°tica: {str(e)}")
                
                elif validation_result.confidence_score >= 0.7:
                    self.logger.info(f"‚úÖ Resposta aprovada pelos guardrails (score: {validation_result.confidence_score:.2f})")
            
            # 8. CONSTRUIR RESPOSTA COM METADADOS CORRETOS
            result = {
                "content": response.content,
                "metadata": {
                    "provider": response.provider.value,
                    "model": response.model,
                    "processing_time": response.processing_time,
                    "tokens_used": response.tokens_used,
                    "data_analysis": needs_data_analysis,
                    "data_loaded": bool(self.current_data_context.get("csv_loaded", False))
                }
            }
            
            # 8. REGISTRAR AGENTES USADOS CORRETAMENTE
            agents_used = ["llm_manager"]
            if needs_data_analysis and self.current_data_context.get("csv_loaded"):
                agents_used.append("embeddings_analyzer")  # Agente de an√°lise via embeddings
            
            return self._enhance_response(result, agents_used)
            
        except Exception as e:
            self.logger.error(f"Erro no LLM Manager: {str(e)}")
            return self._build_response(
                f"‚ùå Erro na an√°lise LLM: {str(e)}",
                metadata={"error": True, "agents_used": ["llm_manager"]}
            )
    
    def _handle_hybrid_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas que requerem m√∫ltiplos agentes."""
        self.logger.info("üîÑ Processando consulta h√≠brida (m√∫ltiplos agentes)")
        
        results = []
        agents_used = []
        
        # Determinar quais agentes s√£o necess√°rios
        query_lower = query.lower()
        
        # CSV + RAG (ex: "analise os dados e busque informa√ß√µes similares")
        if any(kw in query_lower for kw in ['dados', 'csv', 'an√°lise']) and \
           any(kw in query_lower for kw in ['buscar', 'similar', 'contexto']):
            
            # Primeiro: an√°lise CSV se h√° dados
            if "csv" in self.agents and self.current_data_context:
                csv_result = self.agents["csv"].process(query, context)
                results.append(("csv", csv_result))
                agents_used.append("embeddings_analyzer")  # Nome correto do agente
            
            # Segundo: busca RAG
            if "rag" in self.agents:
                rag_result = self.agents["rag"].process(query, context)
                results.append(("rag", rag_result))
                agents_used.append("rag")
        
        # Se nenhum resultado, usar abordagem padr√£o
        if not results:
            return self._handle_csv_analysis(query, context)
        
        # Combinar resultados
        combined_response = self._combine_agent_responses(results)
        
        return self._build_response(
            combined_response,
            metadata={"agents_used": agents_used, "hybrid_query": True}
        )
    
    def _handle_general_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas gerais usando LLM com intelig√™ncia total - SEM HARDCODING.
        
        O LLM decide tudo:
        - Como responder sauda√ß√µes
        - Como se apresentar
        - Como lidar com perguntas gerais
        - Quando pedir mais informa√ß√µes
        
        Apenas fornecemos a PERSONA (Carlos, agente de dados) via system prompt.
        """
        self.logger.info("üí¨ Processando consulta geral via LLM inteligente")

        # Verificar se LLM est√° dispon√≠vel
        if not self.llm_manager:
            return self._build_response(
                "Sistema LLM n√£o dispon√≠vel no momento. Por favor, tente novamente mais tarde.",
                metadata={"agents_used": [], "error": True}
            )

        # Extrair informa√ß√µes do contexto para enriquecer o system prompt
        user_name = None
        if context and 'memory_context' in context:
            user_name = context['memory_context'].get('user_profile', {}).get('name')
        
        # Se n√£o tem nome salvo, tentar extrair da mensagem atual
        if not user_name:
            user_name = self._extract_user_name(query)
            if user_name and context is not None:
                context['user_name'] = user_name  # Propagar para salvar na mem√≥ria

        # Informa√ß√µes do dataset (se dispon√≠vel)
        dataset_info = self._get_dataset_info()
        
        # Hor√°rio contextual
        greeting_time = self._get_contextual_greeting()

        # Verificar se √© primeira intera√ß√£o (para apresenta√ß√£o completa)
        is_first_interaction = False
        if context and 'memory_context' in context:
            conversations = context['memory_context'].get('recent_conversations', [])
            is_first_interaction = len(conversations) == 0
        
        # Resumo seguro de fatos conversacionais e hist√≥rico
        facts_summary = ""
        semantic_memory = ""
        if context and 'memory_context' in context:
            mm = context['memory_context']
            facts_dict = mm.get('conversational_facts') or (
                mm.get('preferences', {}).get('conversational_facts') if isinstance(mm.get('preferences'), dict) else None
            )
            if isinstance(facts_dict, dict):
                facts_summary = self._facts_to_safe_summary(facts_dict)

            # Construir resumo de longo prazo: todas as conversas recentes + perfil
            try:
                recent = mm.get('recent_conversations', [])
                if isinstance(recent, list) and recent:
                    # Sumarizar todas as intera√ß√µes de usu√°rio (n√£o apenas √∫ltimas 3)
                    user_msgs = [m.get('content', '') for m in recent if m.get('role') == 'user']
                    if user_msgs:
                        # Limitar tamanho total para n√£o poluir o prompt
                        summary_lines = [f"- {t[:100]}" for t in user_msgs[-10:]]
                        semantic_memory = "\n".join(summary_lines)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao sumarizar mem√≥rias recentes: {e}")
        
        # üõ°Ô∏è SYSTEM PROMPT COM GUARDRAILS E PERSONA
        # Adicionar nome do usu√°rio se conhecido
        user_greeting = f", {user_name}" if user_name else ""
        
        system_prompt = f"""Voc√™ √© Carlos, Coordenador Central do Sistema Multiagente EDA AI Minds.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéØ SUA FUN√á√ÉO ESPEC√çFICA (ORCHESTRATOR AGENT)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

VOC√ä √â O AGENTE COORDENADOR que:
- üß≠ Roteia queries para agentes especializados (RAG Data, NFe Tax Specialist, etc)
- üí¨ Responde sauda√ß√µes e consultas gerais sobre o sistema
- ü§ù Interage diretamente com usu√°rios em conversas gerais
- üìã Extrai informa√ß√µes do usu√°rio (nome, prefer√™ncias)
- üß† Mant√©m contexto conversacional e mem√≥ria de intera√ß√µes

QUANDO VOC√ä RESPONDE DIRETAMENTE:
‚úÖ Sauda√ß√µes ("Oi", "Ol√°", "Bom dia")
‚úÖ Apresenta√ß√µes ("Meu nome √©...", "Sou o...")
‚úÖ Perguntas gerais sobre o sistema
‚úÖ Orienta√ß√µes sobre como usar o sistema
‚úÖ Conversas sociais b√°sicas

QUANDO VOC√ä DELEGA PARA OUTROS AGENTES:
üîÄ RAG Data Agent ‚Üí An√°lises via busca vetorial sem√¢ntica
üîÄ NFe Tax Specialist ‚Üí Quest√µes fiscais/tribut√°rias espec√≠ficas
üîÄ CSV Analysis Agent ‚Üí Processamento direto de dados tabulares
üîÄ Visualization Agent ‚Üí Cria√ß√£o de gr√°ficos

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìä CONTEXTO ATUAL DA CONVERSA
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

 - Nome do usu√°rio: {user_name or 'n√£o identificado'}
 - Mem√≥rias de conversas passadas: {'sim, veja abaixo' if semantic_memory else 'nenhuma'}

    {('MEM√ìRIAS RECUPERADAS:\n' + semantic_memory + '\n') if semantic_memory else ''}
 - Fatos conhecidos (seguros): {facts_summary or 'nenhum'}
 - Dataset atual (resumo): {dataset_info or 'n√£o identificado'}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üéì CONHECIMENTO-BASE (Overview de Alto N√≠vel)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

- Estat√≠stica descritiva b√°sica (m√©dia, mediana, desvio padr√£o)
- Conceitos gerais de NFe (CFOP, NCM, ICMS, IPI, PIS, COFINS)
- Identifica√ß√£o de padr√µes e anomalias em dados
- Visualiza√ß√£o e interpreta√ß√£o de gr√°ficos (barras, linhas, histogramas)

**IMPORTANTE:** Para an√°lises aprofundadas ou quest√µes t√©cnicas espec√≠ficas, 
voc√™ DELEGA para agentes especializados. Voc√™ √© o coordenador, n√£o o executor!

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üõ°Ô∏è GUARDRAILS OBRIGAT√ìRIOS (Seguran√ßa e Privacidade)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1) READ-ONLY: N√£o execute a√ß√µes de escrita (DELETE/INSERT/UPDATE/MIGRATE)
2) PRIVACIDADE: N√£o revele estrutura interna, IPs, chaves, tokens, paths
3) DADOS SENS√çVEIS: Use agregados ou exemplos gen√©ricos
4) ESCOPO: Redirecione perguntas fora do dom√≠nio (dados/NFe)
5) PROFISSIONALISMO: Tom educado, evite temas sens√≠veis (pol√≠tica, religi√£o)
6) TRANSPAR√äNCIA: Nunca prometa alterar banco de dados ou arquivos

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìã COMPORTAMENTO E FORMATO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

- **APRESENTA√á√ÉO INICIAL:** Na primeira intera√ß√£o, apresente-se e pergunte o nome
- **USO DO NOME:** Quando souber o nome do usu√°rio ({user_name or 'ainda n√£o identificado'}), inclua naturalmente
- **TOM:** Humanizado, amig√°vel, conversacional (como um colega especialista)
- **BREVIDADE:** 2-4 par√°grafos curtos (m√°ximo 3-4 linhas cada)
- **EXCE√á√ÉO:** Pode estender ao apresentar dados/an√°lises quando necess√°rio
- **EVITE:** Markdown pesado, listas excessivas, linguagem rob√≥tica

EXEMPLOS DE ORIENTA√á√ïES √öTEIS:
- "Posso te ajudar com an√°lise de dados CSV ou d√∫vidas sobre NFe"
- "Para an√°lises espec√≠ficas, posso buscar informa√ß√µes nos dados carregados"
- "Quest√µes tribut√°rias? Tenho especialistas para te ajudar!"

Instru√ß√£o final: Seja o anfitri√£o acolhedor e o coordenador eficiente do sistema!"""

        try:
            config = LLMConfig(temperature=0.3, max_tokens=400)
            llm_response = self.llm_manager.chat(query, config, system_prompt=system_prompt)
            
            if llm_response.success and llm_response.content:
                return self._build_response(
                    llm_response.content,
                    metadata={
                        "agents_used": ["llm_manager"],
                        "greeting": True,
                        "provider": llm_response.provider.value,
                        "model": llm_response.model,
                        "tokens_used": llm_response.tokens_used,
                        "processing_time": llm_response.processing_time
                    }
                )
            else:
                raise RuntimeError(llm_response.error or "LLM retornou resposta vazia")
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro no LLM: {str(e)}")
            # Fallback m√≠nimo apenas se LLM falhar completamente
            fallback = "Sou Carlos, agente de an√°lise de dados. Como posso ajudar?"
            return self._build_response(
                fallback,
                metadata={"agents_used": [], "fallback": True, "error": str(e)}
            )

    def _facts_to_safe_summary(self, facts: Dict[str, Any]) -> str:
        """Gera um resumo seguro (sem PII sens√≠vel) dos fatos para o system prompt."""
        if not facts:
            return ""
        non_sensitive_keys = [
            'name', 'city', 'state', 'company', 'role', 'preferred_chart', 'preferred_metric',
            'period_start', 'period_end'
        ]
        shown = []
        for k in non_sensitive_keys:
            v = facts.get(k)
            if v:
                shown.append(f"{k}: {v}")
        return ", ".join(shown)

    def _extract_conversational_facts(self, text: str) -> Dict[str, Any]:
        """Extrai fatos conversacionais estruturados a partir do texto do usu√°rio (heur√≠stica)."""
        try:
            facts: Dict[str, Any] = {}
            if not text:
                return facts
            t = text.strip()

            # Nome
            name = self._extract_user_name(t)
            if name:
                facts['name'] = name

            # E-mail
            email_match = re.search(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", t)
            if email_match:
                facts['email'] = email_match.group(0)

            # Telefone BR simples
            phone_match = re.search(r"\b\(?(?:\d{2})\)?\s?(?:9?\d{4})[- ]?\d{4}\b", t)
            if phone_match:
                facts['phone'] = phone_match.group(0)

            # CPF/CNPJ
            cpf_match = re.search(r"\b\d{3}\.\d{3}\.\d{3}-\d{2}\b|\b\d{11}\b", t)
            if cpf_match:
                facts['cpf'] = cpf_match.group(0)
            cnpj_match = re.search(r"\b\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}\b|\b\d{14}\b", t)
            if cnpj_match:
                facts['cnpj'] = cnpj_match.group(0)

            # Cidade / Estado
            city_match = re.search(r"(?:moro em|sou de|estou em)\s+([A-Z√Ä-√ö][a-z√†-√∫]+(?:\s+[A-Z√Ä-√ö][a-z√†-√∫]+)?)", t, re.IGNORECASE)
            if city_match:
                facts['city'] = city_match.group(1)
            state_match = re.search(r"(?:estado|UF)\s*(?:de|:)\s*([A-Z]{2})\b", t, re.IGNORECASE)
            if state_match:
                facts['state'] = state_match.group(1).upper()

            # Empresa / Cargo
            company_match = re.search(r"(?:trabalho na|na empresa|minha empresa √©|sou da)\s+([A-Z0-9][\w\-\.&\s]{2,})", t, re.IGNORECASE)
            if company_match:
                facts['company'] = company_match.group(1).strip()
            role_match = re.search(r"(?:sou|atuo como|cargo de)\s+([A-Za-z√Ä-√∫\s]{2,})", t, re.IGNORECASE)
            if role_match:
                facts['role'] = role_match.group(1).strip()

            # Per√≠odo / Datas
            date_iso = re.findall(r"\b\d{4}-\d{2}-\d{2}\b", t)
            date_br = re.findall(r"\b\d{2}/\d{2}/\d{4}\b", t)
            if len(date_iso) >= 2:
                facts['period_start'] = date_iso[0]
                facts['period_end'] = date_iso[1]
            elif len(date_br) >= 2:
                facts['period_start'] = date_br[0]
                facts['period_end'] = date_br[1]

            # C√≥digos fiscais
            cfop_match = re.search(r"\bCFOP\s*([0-9]{4})\b", t, re.IGNORECASE)
            if cfop_match:
                facts['cfop'] = cfop_match.group(1)
            ncm_match = re.search(r"\bNCM\s*([0-9]{8})\b", t, re.IGNORECASE)
            if ncm_match:
                facts['ncm'] = ncm_match.group(1)

            # Prefer√™ncias
            if re.search(r"gr[a√°]fico[s]? de barras|bar ?plot", t, re.IGNORECASE):
                facts['preferred_chart'] = 'bar'
            elif re.search(r"histograma", t, re.IGNORECASE):
                facts['preferred_chart'] = 'histogram'
            elif re.search(r"box\s?plot", t, re.IGNORECASE):
                facts['preferred_chart'] = 'boxplot'
            elif re.search(r"dispers[a√£]o|scatter", t, re.IGNORECASE):
                facts['preferred_chart'] = 'scatter'

            if re.search(r"m[e√©]dia|m[e√©]dias", t, re.IGNORECASE):
                facts['preferred_metric'] = 'mean'
            elif re.search(r"mediana", t, re.IGNORECASE):
                facts['preferred_metric'] = 'median'
            elif re.search(r"soma|total", t, re.IGNORECASE):
                facts['preferred_metric'] = 'sum'

            return facts
        except Exception:
            return {}

    def _extract_conversational_facts_llm(self, text: str) -> Dict[str, Any]:
        """Usa LLM para extrair fatos e um mapa de sensibilidade.
        Retorna {'facts': {...}, 'sensitivity': {key: 'sensitive'|'safe'}}.
        """
        try:
            if not self.llm_manager or not text:
                return {}
            import json as _json
            prompt = (
                "Extraia fatos do texto do usu√°rio e classifique cada chave como 'safe' ou 'sensitive'.\n"
                "Responda APENAS em JSON com as chaves: facts (objeto) e sensitivity (objeto),\n"
                "onde sensitivity[k] ‚àà {'safe','sensitive'}.\n\n"
                f"Texto: {text}\n\n"
                "Exemplo de resposta: {\"facts\": {\"city\": \"S√£o Paulo\"}, \"sensitivity\": {\"city\": \"safe\"}}"
            )
            config = LLMConfig(temperature=0.1, max_tokens=300)
            resp = self.llm_manager.chat(prompt, config)
            if not resp.success or not resp.content:
                return {}
            content = resp.content.strip()
            # Sanitiza trechos n√£o-JSON
            try:
                data = _json.loads(content)
            except Exception:
                # tenta localizar primeiro bloco JSON
                start = content.find('{')
                end = content.rfind('}')
                if start != -1 and end != -1:
                    data = _json.loads(content[start:end+1])
                else:
                    return {}
            facts = data.get('facts') or {}
            sensitivity = data.get('sensitivity') or {}
            if not isinstance(facts, dict) or not isinstance(sensitivity, dict):
                return {}
            return {'facts': facts, 'sensitivity': sensitivity}
        except Exception:
            return {}

    def _partition_facts_safe_sensitive(self, facts: Dict[str, Any], sensitivity: Optional[Dict[str, str]] = None) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """Separa fatos entre seguros e sens√≠veis combinando heur√≠stica + mapa do LLM."""
        sensitive_keys_default = {"email", "phone", "cpf", "cnpj", "address", "cep"}
        safe: Dict[str, Any] = {}
        sensitive: Dict[str, Any] = {}
        for k, v in facts.items():
            if v is None or v == "":
                continue
            mark = (sensitivity or {}).get(k)
            if mark == 'sensitive' or k in sensitive_keys_default:
                sensitive[k] = v
            else:
                safe[k] = v
        return safe, sensitive
    
    def _handle_unknown_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas de tipo desconhecido."""
        self.logger.warning(f"ü§î Consulta de tipo desconhecido: {query[:50]}...")
        
        response = f"""ü§î **N√£o consegui identificar o tipo da sua consulta**

**Sua consulta:** "{query}"

**Posso te ajudar com:**
‚Ä¢ üìä **An√°lise de dados:** "analise o arquivo dados.csv"
‚Ä¢ üîç **Busca sem√¢ntica:** "busque informa√ß√µes sobre fraude"
‚Ä¢ üìÅ **Carregar dados:** use context={{"file_path": "arquivo.csv"}}

**Reformule sua pergunta ou seja mais espec√≠fico sobre o que precisa.**
"""
        
        return self._build_response(response, metadata={"agents_used": [], "unknown_query": True})
    
    def _combine_agent_responses(self, results: List[Tuple[str, Dict[str, Any]]]) -> str:
        """Combina respostas de m√∫ltiplos agentes em uma resposta coesa."""
        if not results:
            return "Nenhum resultado dispon√≠vel."
        
        combined = "üîÑ **Resposta Consolidada de M√∫ltiplos Agentes**\n\n"
        
        for agent_name, result in results:
            agent_display = {
                "csv": "üìä **An√°lise CSV**",
                "rag": "üîç **Busca Sem√¢ntica**"
            }.get(agent_name, f"ü§ñ **{agent_name.upper()}**")
            
            combined += f"{agent_display}\n"
            combined += f"{result.get('content', 'Sem conte√∫do')}\n\n"
            combined += "‚îÄ" * 50 + "\n\n"
        
        return combined.rstrip("‚îÄ\n ")
    
    def _enhance_response(self, agent_result: Dict[str, Any], agents_used: List[str]) -> Dict[str, Any]:
        """Melhora resposta do agente com informa√ß√µes do orquestrador."""
        if not agent_result:
            return self._build_response("Erro: resposta vazia do agente", metadata={"error": True})
        
        # Preservar conte√∫do original
        enhanced = agent_result.copy()
        
        # Adicionar informa√ß√µes do orquestrador
        if "metadata" not in enhanced:
            enhanced["metadata"] = {}
        
        # CORRE√á√ÉO: Registrar agentes usados no n√≠vel principal da metadata
        enhanced["metadata"]["agents_used"] = agents_used
        enhanced["metadata"]["orchestrator"] = {
            "conversation_length": len(self.conversation_history),
            "has_data_context": bool(self.current_data_context)
        }
        
        return enhanced
    
    def _get_system_status(self) -> Dict[str, Any]:
        """Retorna status completo do sistema."""
        status_info = {
            "agents": {},
            "data_context": bool(self.current_data_context),
            "conversation_history": len(self.conversation_history)
        }
        
        # Status dos agentes
        for name, agent in self.agents.items():
            status_info["agents"][name] = {
                "available": True,
                "name": agent.name,
                "description": agent.description
            }
        
        # Status do data processor
        if self.data_processor:
            status_info["data_processor"] = {"available": True}
        
        # Informa√ß√µes sobre dados carregados
        data_info = ""
        if self.current_data_context:
            file_path = self.current_data_context.get('file_path', 'N/A')
            data_info = f"\nüìÅ **Dados Carregados:** {file_path}"
        
        response = f"""‚ö° **Status do Sistema EDA AI Minds**

ü§ñ **Agentes Dispon√≠veis:** {len(self.agents)}
{chr(10).join(f'‚Ä¢ {name.upper()}: {agent.description}' for name, agent in self.agents.items())}

üíæ **Data Processor:** {'‚úÖ Ativo' if self.data_processor else '‚ùå Inativo'}
üí¨ **Hist√≥rico:** {len(self.conversation_history)} intera√ß√µes{data_info}

üöÄ **Sistema Operacional e Pronto!**
"""
        
        return self._build_response(response, metadata=status_info)
    
    def _get_help_response(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes de ajuda completas."""
        help_text = """üìö **Guia de Uso do Sistema EDA AI Minds**

## üîç **Tipos de Consulta**

### üìä **An√°lise de Dados CSV**
```python
# Carregar arquivo
context = {"file_path": "dados.csv"}
query = "carregue os dados"

# An√°lises
"fa√ßa um resumo dos dados"
"mostre as correla√ß√µes"
"analise fraudes"
"crie visualiza√ß√µes"
```

### üß† **Busca Sem√¢ntica (RAG)**
```python
"busque informa√ß√µes sobre detec√ß√£o de fraude"
"encontre dados similares a transa√ß√µes suspeitas"
"qual o contexto sobre an√°lise de risco?"
```

### üìÅ **Carregamento de Dados**
```python
"carregar arquivo CSV"
"importar dados"
"gerar dados sint√©ticos"
```

## üí° **Dicas**
‚Ä¢ Seja espec√≠fico nas consultas
‚Ä¢ Use contexto para fornecer arquivos
‚Ä¢ Combine diferentes tipos de an√°lise
‚Ä¢ Pergunte sobre status do sistema

**Exemplo Completo:**
```python
# 1. Carregar dados
context = {"file_path": "fraude.csv"}
"carregue e analise os dados"

# 2. An√°lise espec√≠fica  
"mostre correla√ß√µes entre valor e fraude"

# 3. Busca contextual
"busque padr√µes similares na base de conhecimento"
```
"""
        
        return self._build_response(help_text, metadata={"help": True, "agents_used": []})
    
    def _get_contextual_greeting(self) -> str:
        """Retorna sauda√ß√£o contextual baseada no hor√°rio local."""
        from datetime import datetime
        
        hour = datetime.now().hour
        
        if 5 <= hour < 12:
            return "Bom dia"
        elif 12 <= hour < 18:
            return "Boa tarde"
        else:
            return "Boa noite"
    
    def _extract_user_name(self, query: str) -> Optional[str]:
        """Extrai nome do usu√°rio da mensagem (ex: 'Meu nome √© Jo√£o', 'Sou a Maria')."""
        import re
        
        # Padr√µes para extra√ß√£o de nome
        patterns = [
            r'(?:meu nome (?:√©|e))\s+([A-Z√Ä√Å√Ç√É√Ñ√Ö√á√à√â√ä√ã√å√ç√é√è√ë√í√ì√î√ï√ñ√ô√ö√õ√ú][a-z√†√°√¢√£√§√•√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√π√∫√ª√º]+)',
            r'(?:sou (?:o|a))\s+([A-Z√Ä√Å√Ç√É√Ñ√Ö√á√à√â√ä√ã√å√ç√é√è√ë√í√ì√î√ï√ñ√ô√ö√õ√ú][a-z√†√°√¢√£√§√•√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√π√∫√ª√º]+)',
            r'(?:me chamo)\s+([A-Z√Ä√Å√Ç√É√Ñ√Ö√á√à√â√ä√ã√å√ç√é√è√ë√í√ì√î√ï√ñ√ô√ö√õ√ú][a-z√†√°√¢√£√§√•√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√π√∫√ª√º]+)',
            r'(?:pode me chamar de)\s+([A-Z√Ä√Å√Ç√É√Ñ√Ö√á√à√â√ä√ã√å√ç√é√è√ë√í√ì√î√ï√ñ√ô√ö√õ√ú][a-z√†√°√¢√£√§√•√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√µ√∂√π√∫√ª√º]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None
    
    async def _recall_semantic_memory(self, query: str, limit: int = 5) -> str:
            """Recupera mem√≥rias conversacionais relevantes via busca sem√¢ntica.
        
            Retorna um resumo textual das conversas passadas mais similares √† query atual.
            """
            if not self.has_memory or not self._current_session_id:
                return ""
        
            try:
                # Gera embedding da query atual
                query_embedding = await self.generate_conversation_embedding(query)
                if not query_embedding:
                    return ""
            
                # Busca conversas similares
                similar = await self._memory_manager.search_similar(
                    query_embedding=query_embedding,
                    similarity_threshold=SEMANTIC_MEMORY_SIMILARITY_THRESHOLD,
                    limit=limit,
                    session_id=self._current_session_id
                )
            
                if not similar:
                    return ""
            
                # Formata mem√≥rias recuperadas
                memory_snippets = []
                for i, result in enumerate(similar, 1):
                    snippet = result.source_text[:300]  # Trunca para evitar overflow
                    memory_snippets.append(f"[Mem√≥ria {i}, similaridade {result.similarity:.2f}] {snippet}")
            
                memory_text = "\n\n".join(memory_snippets)
                self.logger.info(f"üîç Recuperadas {len(similar)} mem√≥rias sem√¢nticas relevantes")
                return memory_text
            
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao recuperar mem√≥ria sem√¢ntica: {e}")
                return ""
    
    def _get_dataset_info(self) -> str:
        """Obt√©m informa√ß√µes sobre o dataset dispon√≠vel atrav√©s da LLM, sem hardcode.
        
        A LLM analisa os dados dispon√≠veis e descreve de forma inteligente e segura,
        respeitando os guardrails de n√£o expor nomes de tabelas, campos ou arquivos.
        """
        if not SUPABASE_CLIENT_AVAILABLE or not supabase or not self.llm_manager:
            return ""  # Retorna vazio, deixa LLM decidir como responder
        
        try:
            # Buscar amostra de dados para a LLM analisar (texto sem√¢ntico + metadados)
            result = supabase.table('embeddings').select('chunk_text, metadata').limit(5).execute()
            if not result.data:
                return ""  # Sem dados, deixa LLM decidir

            # Preparar amostras sanitizadas, evitando PII e detalhes t√©cnicos
            import re
            snippets = []
            for row in result.data:
                txt = row.get('chunk_text') or ''
                if not isinstance(txt, str):
                    continue
                # Remover e mascarar poss√≠veis PII/identificadores sens√≠veis
                txt = re.sub(r"\b\d{3}\.\d{3}\.\d{3}-\d{2}\b", "[CPF]", txt)  # CPF
                txt = re.sub(r"\b\d{2}\.\d{3}\.\d{3}/\d{4}-\d{2}\b", "[CNPJ]", txt)  # CNPJ
                txt = re.sub(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", "[EMAIL]", txt)  # emails
                txt = re.sub(r"https?://\S+", "[URL]", txt)  # URLs
                txt = re.sub(r"[A-Za-z]:\\\\[^\s]+|/[^\s]+", "[PATH]", txt)  # paths
                # Reduzir n√∫meros muito longos (ex.: chaves de acesso)
                txt = re.sub(r"\b\d{8,}\b", "[NUM]", txt)
                # Truncar para evitar verbosidade
                txt = txt.strip().replace("\n", " ")[:280]
                if txt:
                    snippets.append(txt)

            # Remover duplicatas mantendo ordem
            seen = set()
            unique_snippets = []
            for s in snippets:
                if s not in seen:
                    unique_snippets.append(s)
                    seen.add(s)
            # Limitar a 3-5 trechos
            unique_snippets = unique_snippets[:5]

            # LLM analisa e descreve de forma segura, sem qualquer pista ou fallback fixo
            prompt = (
                "Analise estes trechos (amostra) do dataset e descreva o DOM√çNIO/TEMA de forma gen√©rica e segura.\n\n"
                f"Trechos:\n- " + "\n- ".join(unique_snippets) + "\n\n"
                "REGRAS:\n"
                "- N√ÉO mencione nomes de tabelas/colunas/arquivos ou paths\n"
                "- N√ÉO exponha PII (e-mails, CPFs, CNPJs, chaves)\n"
                "- Responda em UMA frase curta (6‚Äì12 palavras), apenas com o tema (ex.: 'dados fiscais de notas eletr√¥nicas')\n"
                "- Seja objetivo e n√£o inclua justificativas"
            )

            config = LLMConfig(temperature=0.2, max_tokens=50)
            response = self.llm_manager.chat(prompt, config)
            if response.success and response.content:
                return response.content.strip()
            return ""
            
        except Exception as e:
            self.logger.warning(f"Erro ao obter informa√ß√µes do dataset: {e}")
            return ""  # Erro, deixa LLM principal decidir
    
    # ========================================================================
    # M√âTODOS DE PROCESSAMENTO COM MEM√ìRIA
    # ========================================================================
    
    async def process_with_persistent_memory(self, query: str, context: Optional[Dict[str, Any]] = None,
                                           session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Processa consulta utilizando sistema de mem√≥ria persistente Supabase.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional
            session_id: ID da sess√£o (inicializa se None)
            
        Returns:
            Resposta processada com persist√™ncia de mem√≥ria
        """
        self.logger.info(f"üß† Processando com mem√≥ria persistente: '{query[:50]}...'")
        
        try:
            # 1. Inicializar sess√£o de mem√≥ria se necess√°rio
            if session_id and self.has_memory:
                if not self._current_session_id or self._current_session_id != session_id:
                    await self.init_memory_session(session_id)
            elif not self._current_session_id and self.has_memory:
                session_id = await self.init_memory_session()
            
            # 2. Recuperar contexto de mem√≥ria
            memory_context = {}
            if self.has_memory and self._current_session_id:
                memory_context = await self.recall_conversation_context()
                self.logger.debug(f"Contexto de mem√≥ria recuperado: {len(memory_context.get('recent_conversations', []))} intera√ß√µes")
                
                # Recuperar explicitamente user_profile do Supabase (long-term memory)
                try:
                    if self._memory_manager:
                        user_profile_ctx = await self._memory_manager.get_context(
                            self._current_session_id,
                            ContextType.DATA,
                            'user_profile'
                        )
                        if user_profile_ctx and isinstance(user_profile_ctx.context_data, dict):
                            memory_context['user_profile'] = user_profile_ctx.context_data
                            self.logger.info(f"‚úÖ Perfil do usu√°rio recuperado do Supabase: {user_profile_ctx.context_data.get('name')}")
                except Exception as e:
                    self.logger.warning(f"Erro ao recuperar perfil do Supabase: {e}")
                
                # Fallback: verificar hist√≥rico de mensagens se n√£o encontrou no context
                if 'user_profile' not in memory_context:
                    try:
                        for msg in memory_context.get('recent_conversations', []):
                            msg_metadata = msg.get('metadata', {})
                            if 'user_name' in msg_metadata:
                                memory_context['user_profile'] = {'name': msg_metadata['user_name']}
                                self.logger.info(f"‚úÖ Perfil do usu√°rio recuperado da mem√≥ria de mensagens: {msg_metadata['user_name']}")
                                break
                    except Exception as e:
                        self.logger.warning(f"Erro ao recuperar perfil do usu√°rio das mensagens: {e}")
                
                # Recuperar fatos conversacionais salvos (long-term)
                try:
                    if self._memory_manager:
                        facts_ctx = await self._memory_manager.get_context(
                            self._current_session_id,
                            ContextType.PREFERENCES,
                            'conversational_facts'
                        )
                        if facts_ctx and isinstance(facts_ctx.context_data, dict):
                            memory_context['conversational_facts'] = facts_ctx.context_data
                            self.logger.debug(f"‚úÖ Fatos conversacionais recuperados: {list(facts_ctx.context_data.keys())}")
                except Exception as e:
                    self.logger.warning(f"Erro ao recuperar fatos conversacionais: {e}")
                
                # NOVO: Extrair e persistir nome do usu√°rio mesmo quando 'context' √© None
                try:
                    detected_name = self._extract_user_name(query) if query else None
                    name_in_memory = memory_context.get('user_profile', {}).get('name') if memory_context else None
                    if detected_name and not name_in_memory:
                        # Atualiza contexto em mem√≥ria imediatamente
                        memory_context.setdefault('user_profile', {})['name'] = detected_name
                        if context is None:
                            context = {}
                        context['user_name'] = detected_name
                        # Persiste o perfil do usu√°rio na mem√≥ria do Supabase
                        await self.remember_data_context(
                            {'name': detected_name, 'first_seen': self._get_timestamp()},
                            "user_profile"
                        )
                        self.logger.info(f"‚úÖ Nome do usu√°rio detectado e salvo na mem√≥ria: {detected_name}")
                except Exception as e:
                    self.logger.warning(f"Erro ao detectar/persistir nome do usu√°rio: {e}")
                
                # NOVO: Extrair e persistir fatos conversacionais (prefer√™ncias, entidades, per√≠odo, IDs)
                try:
                    # 1) Heur√≠stica regex
                    facts_regex = self._extract_conversational_facts(query or "")
                    # 2) Extra√ß√£o via LLM (se dispon√≠vel)
                    facts_llm = {}
                    sensitivity_map = {}
                    if self.llm_manager and query:
                        try:
                            res = self._extract_conversational_facts_llm(query)
                            if isinstance(res, dict):
                                facts_llm = res.get('facts', {}) or {}
                                sensitivity_map = res.get('sensitivity', {}) or {}
                        except Exception:
                            pass

                    # Merge de fatos (LLM tem preced√™ncia sobre regex)
                    merged_all = {}
                    merged_all.update(facts_regex)
                    merged_all.update(facts_llm)

                    if merged_all:
                        # Carregar safe e sensitive j√° existentes
                        existing_safe = None
                        existing_sensitive = None
                        try:
                            if self._memory_manager:
                                existing_safe = await self._memory_manager.get_context(
                                    self._current_session_id,
                                    ContextType.PREFERENCES,
                                    'conversational_facts'
                                )
                                existing_sensitive = await self._memory_manager.get_context(
                                    self._current_session_id,
                                    ContextType.LEARNING,
                                    'sensitive_facts'
                                )
                        except Exception:
                            existing_safe = None
                            existing_sensitive = None

                        # Separar safe x sensitive
                        safe_facts, sensitive_facts = self._partition_facts_safe_sensitive(merged_all, sensitivity_map)

                        # Mesclar com existentes
                        if existing_safe and isinstance(existing_safe.context_data, dict):
                            safe_facts = {**existing_safe.context_data, **{k: v for k, v in safe_facts.items() if v}}
                        if existing_sensitive and isinstance(existing_sensitive.context_data, dict):
                            sensitive_facts = {**existing_sensitive.context_data, **{k: v for k, v in sensitive_facts.items() if v}}

                        # Persistir em Supabase
                        if self._memory_manager:
                            if safe_facts:
                                await self._memory_manager.save_context(
                                    self._current_session_id,
                                    ContextType.PREFERENCES,
                                    'conversational_facts',
                                    safe_facts,
                                    priority=2
                                )
                                memory_context['conversational_facts'] = safe_facts
                            if sensitive_facts:
                                await self._memory_manager.save_context(
                                    self._current_session_id,
                                    ContextType.LEARNING,
                                    'sensitive_facts',
                                    sensitive_facts,
                                    priority=3
                                )

                        saved_keys = list(safe_facts.keys()) + [f"sensitive:{k}" for k in sensitive_facts.keys()]
                        if saved_keys:
                            self.logger.info(f"‚úÖ Fatos conversacionais salvos: {saved_keys}")
                except Exception as e:
                    self.logger.warning(f"Erro ao extrair/persistir fatos conversacionais: {e}")
                
                # Mescla contexto de mem√≥ria com contexto atual
                if context:
                    context.update({"memory_context": memory_context})
                else:
                    context = {"memory_context": memory_context}
            
            # 3. Verificar cache de an√°lises
            analysis_cache_key = None
            if context and context.get('file_path'):
                analysis_cache_key = f"analysis_{hash(query + str(context.get('file_path')))}"
                cached_result = await self.recall_cached_analysis(analysis_cache_key)
                if cached_result:
                    self.logger.info("üì¶ Resultado recuperado do cache de an√°lises")
                    cached_result['metadata']['from_cache'] = True
                    return cached_result
            
            # 4. Processar consulta usando vers√£o ass√≠ncrona (evita coroutines n√£o aguardadas)
            result = await self._process_async(query, context)
            
            # 5. Salvar intera√ß√£o na mem√≥ria persistente
            if self.has_memory and self._current_session_id:
                # Preparar metadata estendida
                extended_metadata = result.get('metadata', {}).copy()
                
                # Se nome do usu√°rio foi extra√≠do, adicionar √† mem√≥ria
                user_name_to_save = None
                if context and 'user_name' in context:
                    user_name_to_save = context['user_name']
                elif query:
                    # Fallback: tentar extrair do texto novamente
                    user_name_to_save = self._extract_user_name(query)

                if user_name_to_save:
                    extended_metadata['user_name'] = user_name_to_save
                    # Salvar perfil do usu√°rio no contexto de mem√≥ria, se necess√°rio
                    if not memory_context.get('user_profile') or not memory_context['user_profile'].get('name'):
                        memory_context['user_profile'] = {'name': user_name_to_save}
                        await self.remember_data_context(
                            {'name': user_name_to_save, 'first_seen': self._get_timestamp()},
                            "user_profile"
                        )
                        self.logger.info(f"‚úÖ Nome do usu√°rio salvo/atualizado na mem√≥ria: {user_name_to_save}")
                
                await self.remember_interaction(
                    query=query,
                    response=result.get('content', str(result)),
                    metadata=extended_metadata
                )
                
                # üÜï MEM√ìRIA DE LONGO PRAZO: Persistir embeddings automaticamente a cada 5 turnos
                try:
                    turn_count = len(memory_context.get('recent_conversations', [])) + 1
                    if turn_count % 5 == 0:
                        self.logger.info(f"üß† Persistindo mem√≥ria sem√¢ntica (turno {turn_count})...")
                        success = await self.persist_conversation_memory(hours_back=24)
                        if success:
                            self.logger.info("‚úÖ Mem√≥ria sem√¢ntica persistida com sucesso")
                        else:
                            self.logger.warning("‚ö†Ô∏è Falha ao persistir mem√≥ria sem√¢ntica")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao persistir mem√≥ria sem√¢ntica: {e}")
                
                # 6. Cachear resultado de an√°lise se aplic√°vel
                if analysis_cache_key and result.get('metadata', {}).get('query_type') in ['csv_analysis', 'llm_analysis']:
                    await self.remember_analysis_result(analysis_cache_key, result, expiry_hours=24)
                
                # 7. Salvar contexto de dados se carregado
                if context and context.get('file_path'):
                    data_context = {
                        'file_path': context['file_path'],
                        'last_query': query,
                        'timestamp': self._get_timestamp()
                    }
                    await self.remember_data_context(data_context, "current_data")
            
            # 8. Adicionar informa√ß√µes de mem√≥ria √† resposta
            if self.has_memory:
                result.setdefault('metadata', {})['session_id'] = self._current_session_id
                result.setdefault('metadata', {})['memory_enabled'] = True
                
                # Estat√≠sticas de mem√≥ria
                memory_stats = await self.get_memory_stats()
                result.setdefault('metadata', {})['memory_stats'] = memory_stats
            
            # 9. Garantir compatibilidade do campo 'content' (RAGDataAgent retorna 'response')
            if 'response' in result and 'content' not in result:
                result['content'] = result['response']
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro no processamento com mem√≥ria: {e}", exc_info=True)
            # Fallback para processamento sem mem√≥ria sincronamente
            try:
                return self.process(query, context)
            except Exception:
                return self._build_response(f"‚ùå Erro no processamento com mem√≥ria: {str(e)}", metadata={"error": True})
    
    # ========================================================================
    # M√âTODOS DE GEST√ÉO DE MEM√ìRIA PARA COMPATIBILIDADE
    # ========================================================================
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Retorna hist√≥rico completo da conversa (compatibilidade).
        
        DEPRECIADO: Use get_persistent_conversation_history() para mem√≥ria Supabase.
        """
        return self.conversation_history.copy()
    
    async def get_persistent_conversation_history(self, limit: int = 50) -> List[Dict[str, Any]]:
        """Retorna hist√≥rico de conversa√ß√£o da mem√≥ria persistente."""
        if not self.has_memory or not self._current_session_id:
            return self.get_conversation_history()  # Fallback
        
        try:
            conversations = await self.recall_conversation()
            return conversations[:limit]
        except Exception as e:
            self.logger.error(f"Erro ao recuperar hist√≥rico persistente: {e}")
            return self.get_conversation_history()  # Fallback
    
    def clear_conversation_history(self) -> Dict[str, Any]:
        """Limpa hist√≥rico da conversa (compatibilidade).
        
        DEPRECIADO: Use clear_persistent_memory() para mem√≥ria Supabase.
        """
        count = len(self.conversation_history)
        self.conversation_history.clear()
        self.logger.info(f"Hist√≥rico limpo: {count} intera√ß√µes removidas")
        
        return self._build_response(
            f"‚úÖ Hist√≥rico limpo: {count} intera√ß√µes removidas",
            metadata={"cleared_count": count}
        )
    
    async def clear_persistent_memory(self) -> Dict[str, Any]:
        """Limpa mem√≥ria persistente da sess√£o atual."""
        if not self.has_memory or not self._current_session_id:
            return self.clear_conversation_history()  # Fallback
        
        try:
            # Implementar limpeza via memory manager se necess√°rio
            # Por enquanto, inicia nova sess√£o
            old_session = self._current_session_id
            await self.init_memory_session()
            
            return self._build_response(
                f"‚úÖ Mem√≥ria persistente limpa. Nova sess√£o: {self._current_session_id}",
                metadata={"old_session": old_session, "new_session": self._current_session_id}
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao limpar mem√≥ria persistente: {e}")
            return self.clear_conversation_history()  # Fallback
    
    def clear_data_context(self) -> Dict[str, Any]:
        """Limpa contexto de dados carregados (compatibilidade).
        
        DEPRECIADO: Use clear_persistent_data_context() para mem√≥ria Supabase.
        """
        if self.current_data_context:
            file_path = self.current_data_context.get('file_path', 'N/A')
            self.current_data_context.clear()
            self.logger.info(f"Contexto de dados limpo: {file_path}")
            
            return self._build_response(
                f"‚úÖ Contexto de dados limpo: {file_path}",
                metadata={"cleared_data": file_path}
            )
        else:
            return self._build_response(
                "‚ÑπÔ∏è Nenhum contexto de dados para limpar",
                metadata={"no_data_context": True}
            )
    
    async def clear_persistent_data_context(self) -> Dict[str, Any]:
        """Limpa contexto de dados da mem√≥ria persistente."""
        if not self.has_memory or not self._current_session_id:
            return self.clear_data_context()  # Fallback
        
        try:
            # Aqui implementar√≠amos limpeza espec√≠fica do contexto de dados
            # Por simplicidade, vamos usar o m√©todo de compatibilidade
            result = self.clear_data_context()
            
            # Tamb√©m limpar do sistema de mem√≥ria se houver implementa√ß√£o espec√≠fica
            self.logger.info("Contexto de dados persistente limpo")
            return result
            
        except Exception as e:
            self.logger.error(f"Erro ao limpar contexto de dados persistente: {e}")
            return self.clear_data_context()  # Fallback
    
    def _update_data_context_from_csv_result(self, csv_result: Dict[str, Any], context: Dict[str, Any]) -> None:
        """Atualiza contexto de dados com resultado da an√°lise CSV."""
        try:
            csv_content = csv_result.get("content", "")
            
            # Extrair informa√ß√µes b√°sicas do resultado CSV
            data_info = {
                "file_path": context.get("file_path", ""),
                "csv_loaded": True,
                "structure_analyzed": True,
                "csv_analysis": csv_content
            }
            
            # Tentar extrair informa√ß√µes espec√≠ficas do conte√∫do
            if "Colunas:" in csv_content:
                # Extrair lista de colunas se dispon√≠vel
                lines = csv_content.split('\n')
                for i, line in enumerate(lines):
                    if "Colunas:" in line and i + 1 < len(lines):
                        columns_info = lines[i + 1].strip()
                        data_info["columns_summary"] = columns_info
                        break
            
            if "Shape:" in csv_content:
                # Extrair informa√ß√µes de shape
                lines = csv_content.split('\n')
                for line in lines:
                    if "Shape:" in line:
                        shape_info = line.replace("Shape:", "").strip()
                        data_info["shape"] = shape_info
                        break
            
            # Atualizar contexto global
            self.current_data_context.update(data_info)
            self.logger.info(f"‚úÖ Contexto de dados atualizado: {data_info['file_path']}")
            
        except Exception as e:
            self.logger.error(f"Erro ao atualizar contexto de dados: {e}")

    def get_available_agents(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes sobre agentes dispon√≠veis."""
        agents_info = {}
        
        for name, agent in self.agents.items():
            agents_info[name] = {
                "name": agent.name,
                "description": agent.description,
                "class": agent.__class__.__name__
            }
        
        response = "ü§ñ **Agentes Dispon√≠veis**\n\n"
        for name, info in agents_info.items():
            response += f"‚Ä¢ **{name.upper()}**: {info['description']}\n"
        
        return self._build_response(response, metadata={"agents": agents_info})

    def _build_llm_prompt(self, query: str, context: Optional[Dict[str, Any]] = None, needs_data_analysis: bool = False) -> Tuple[str, Optional[str]]:
        """Constr√≥i prompt contextualizado para o LLM Manager.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional (dados, hist√≥rico, etc.)
            needs_data_analysis: Se a consulta requer an√°lise de dados espec√≠ficos
            
        Returns:
            Tuple[str, Optional[str]]: (user_prompt, system_prompt)
        """
        prompt_parts = []
        
        # Instru√ß√£o base diferenciada
        if needs_data_analysis and context and context.get("csv_loaded"):
            prompt_parts.append("""Voc√™ √© um assistente especializado em an√°lise de dados CSV.
Responda com base ESPECIFICAMENTE nos dados carregados fornecidos no contexto.
Use portugu√™s brasileiro e seja preciso e detalhado sobre os dados reais.""")
        else:
            prompt_parts.append("""Voc√™ √© um assistente de an√°lise de dados especializado em CSV e an√°lise estat√≠stica.
Responda de forma clara, precisa e √∫til. Use portugu√™s brasileiro.""")
        
        # Adicionar contexto de dados se dispon√≠vel
        if context:
            if 'file_path' in context:
                prompt_parts.append(f"\nüìä ARQUIVO CARREGADO: {context['file_path']}")
            
            if 'csv_analysis' in context:
                prompt_parts.append(f"\nüìà AN√ÅLISE DOS DADOS:\n{context['csv_analysis']}")
                
            if 'columns_summary' in context:
                prompt_parts.append(f"\nüìã COLUNAS: {context['columns_summary']}")
                
            if 'shape' in context:
                prompt_parts.append(f"\nÔøΩ DIMENS√ïES: {context['shape']}")
        
        # Adicionar a consulta do usu√°rio
        prompt_parts.append(f"\n‚ùì CONSULTA DO USU√ÅRIO: {query}")
        
        # Instru√ß√£o final diferenciada
        if needs_data_analysis and context and context.get("csv_loaded"):
            # üîÑ CORRE√á√ÉO CR√çTICA: Sempre analisar dados CSV ESTRUTURADOS reconstru√≠dos da tabela embeddings
            prompt_parts.append("""\nüéØ INSTRU√á√ïES CR√çTICAS PARA AN√ÅLISE DE DADOS CSV (da tabela embeddings):

ÔøΩ CONTEXTO RECEBIDO:
- Voc√™ recebeu DADOS ESTRUTURADOS (DataFrame) reconstru√≠dos da coluna chunk_text da tabela embeddings
- Esses dados foram parseados como CSV e representam as COLUNAS ORIGINAIS do arquivo CSV carregado
- As estat√≠sticas fornecidas (dtypes, describe, info) refletem os DADOS REAIS, n√£o a estrutura da tabela embeddings

üîç COMO ANALISAR:
1. EXAMINE as COLUNAS listadas na se√ß√£o "AN√ÅLISE DOS DADOS"
2. IDENTIFIQUE os TIPOS DE DADOS usando dtypes:
   - **Num√©ricos**: float64, int64, float32, int32, etc.
   - **Categ√≥ricos**: object, category, bool
   - **Temporais**: datetime64, timedelta
   - **Texto**: object (sem padr√£o num√©rico)

3. USE as ESTAT√çSTICAS FORNECIDAS:
   - Para distribui√ß√µes: count, mean, std, min, max, quartis
   - Para valores √∫nicos: nunique(), value_counts()
   - Para tipos: dtypes expl√≠citos

‚ö†Ô∏è REGRAS CR√çTICAS:
- Use APENAS os dtypes fornecidos para classificar tipos de dados
- N√ÉO confunda com colunas da tabela embeddings (id, chunk_text, created_at, embedding)
- N√ÉO interprete palavras soltas ou descri√ß√µes textuais como se fossem colunas
- Se o contexto mostra "Colunas: ['Time', 'V1', ..., 'Amount', 'Class']", essas s√£o as colunas REAIS
- Seja PRECISO: liste EXATAMENTE as colunas fornecidas, com seus tipos REAIS
- Se a informa√ß√£o n√£o est√° no contexto estruturado, diga que n√£o tem acesso a ela""")
        else:
            prompt_parts.append("\nüéØ Forne√ßa uma resposta √∫til e estruturada:")
        
        # Adicionar corre√ß√µes se dispon√≠veis
        if context and 'correction_prompt' in context:
            prompt_parts.append(f"\n{context['correction_prompt']}")
            prompt_parts.append("\nRefa√ßa sua resposta com os valores corretos fornecidos acima.")
        
        return "\n".join(prompt_parts)